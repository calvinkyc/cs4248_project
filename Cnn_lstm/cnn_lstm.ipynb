{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Pp-VjL_5Xw"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#data_dir =  '/content/drive/MyDrive/Colab Notebooks/'\n",
        "data_dir = './data/'\n",
        "\n",
        "#file to download to run model:  \n",
        "#1) http://nlp.stanford.edu/data/glove.840B.300d.zip for general embedding\n",
        "#2) https://howardhsu.github.io/dataset/ for domain embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ],
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, data_utils\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate word_index list\n",
        "def build_vocab(data_dir, plain = []):\n",
        "  \"\"\"plain is a empty str file which will record all text from official dataset\"\"\"\n",
        "  for fn in os.listdir(data_dir):\n",
        "      if fn.endswith('.xml'):\n",
        "          with open(data_dir+fn) as f:\n",
        "            dom=ET.parse(f)\n",
        "            root=dom.getroot()\n",
        "            for sent in root.iter(\"sentence\"):\n",
        "                text = sent.find('text').text.lower()\n",
        "                token = word_tokenize(text)\n",
        "                plain = plain + token\n",
        "  vocab = sorted(set(plain))\n",
        "  word_idx = {}\n",
        "  for idx, word in enumerate(vocab):\n",
        "        word_idx[word] = idx+1   \n",
        "  return word_idx\n",
        "\n",
        "word_indx = build_vocab(data_dir)"
      ],
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_indx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJVv6N0pTIR6",
        "outputId": "5151b4ef-a6de-485e-cc10-b9c5db0f5eab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7876"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn+\".bin\")\n",
        "    embedding=np.zeros((len(word_idx)+2, dim) )\n",
        "\n",
        "    with open(fn) as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec=l.rstrip().split(' ')\n",
        "            if len(rec)==2: #skip the first line.\n",
        "                continue \n",
        "            # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:] ])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w] ].sum()==0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w] ] = model.get_word_vector(w)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "LiqrfwzMf50x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn = data_dir + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim = 100, emb = True)\n",
        "\n",
        "fn = data_dir + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim = 100, emb = True)\n",
        "\n",
        "fn = data_dir + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim = 300, emb = False)"
      ],
      "metadata": {
        "id": "PnbH1kXllslz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdf54c1-11b3-4d29-a08b-1532d8d11d5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "general_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXde5DS5mjzC",
        "outputId": "dbd1777d-2ebd-4b8f-e4cf-ae61504b5b92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7878, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_data_restaurant(fn, word_idx,  sent_len=83, sent_num=3050):\n",
        "\n",
        "    corpus = []\n",
        "    corpus_tag = []\n",
        "    opsList = []\n",
        "    train_X = np.zeros((sent_num, sent_len), np.int16)\n",
        "\n",
        "    train_y = np.zeros((sent_num, sent_len), np.int16) \n",
        "\n",
        "\n",
        "    dom=ET.parse(fn)\n",
        "    root=dom.getroot()\n",
        "    # iterate the review sentence\n",
        "    for sx, sent in enumerate(root.iter(\"sentence\") ) : \n",
        "        text = sent.find('text').text.lower()\n",
        "        # tokenize the current sentence\n",
        "        token = word_tokenize(text)\n",
        "        corpus.append(token)\n",
        "           \n",
        "        # write word index and tag in train_X and train_X_tag\n",
        "        for wx, word in enumerate(token):\n",
        "            train_X[sx, wx] = word_idx[word]\n",
        "\n",
        "        # create a list for opinions in this sentence\n",
        "        opList = []\n",
        "        # iterate the opinions\n",
        "        for ox, apin in enumerate(sent.iter('aspectTerms') ) :\n",
        "            for ax, opin in enumerate(apin.iter('aspectTerm')):\n",
        "              target, polarity, start, end = opin.attrib['term'], opin.attrib['polarity'], int(opin.attrib['from']), int(opin.attrib['to'])\n",
        "              # find word index (instead of str index) if start,end is not (0,0)\n",
        "              if end != 0:\n",
        "                  if start != 0:\n",
        "                      start = len(word_tokenize(text[:start]))\n",
        "                  end = len(word_tokenize(text[:end]))-1\n",
        "                  # for training only identify aspect word, but not polarity\n",
        "                  train_y[sx, start] = 1\n",
        "                  if end > start:\n",
        "                      train_y[sx, start+1:end] = 2   \n",
        "              opList.append([target, polarity, start, end])\n",
        "          # get a list of opinions attributes\n",
        "        opsList.append(opList)\n",
        "    \n",
        "    return train_X, train_y\n",
        "\n",
        "fn = data_dir + 'Restaurants_Train_v2.xml'\n",
        "X_train_res, y_train_res = create_train_data_restaurant(fn, word_indx,sent_len=100)\n",
        "fn = data_dir + 'Laptop_Train_v2.xml'\n",
        "X_train_lap, y_train_lap = create_train_data_restaurant(fn, word_indx,sent_len=100)\n",
        "fn_test = data_dir + 'Restaurants_Test_Gold.xml'\n",
        "X_test, y_test = create_train_data_restaurant(fn_test, word_indx, sent_len=100)"
      ],
      "metadata": {
        "id": "bITAH2M_J-Js"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not used - try to add a POS tag layer\n",
        "\n",
        "#pos_tag_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP','SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',',','.',':','$','#',\"``\",\"''\",'(',')','-LRB-','-RRB-','HYPH','NFP','SYM','PUNC','AFX','ADD']\n",
        "#pos_len = 53\n",
        "\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "POSdir = data_dir + 'stanford-postagger-full/'\n",
        "\n",
        "def create_pos_tag(fn, POSdir, sent_len=83, sent_num=3050):\n",
        "  pos_tag_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP','SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',',','.',':','$','#',\"``\",\"''\",'(',')','-LRB-','-RRB-','HYPH','NFP','SYM','PUNC','AFX','ADD']\n",
        "  tag_to_num = {tag:i+1 for i, tag in enumerate(sorted(pos_tag_list))}\n",
        "\n",
        "  corpus_tag = []\n",
        "  \n",
        "  train_X_tag = np.zeros((sent_num, sent_len), np.int16)\n",
        "  \n",
        "  dom=ET.parse(fn)\n",
        "  root=dom.getroot()\n",
        "\n",
        "  for sx, sent in enumerate(root.iter(\"sentence\") ) : \n",
        "    text = sent.find('text').text\n",
        "    token = word_tokenize(text)\n",
        "\n",
        "    jar = POSdir+'stanford-postagger.jar'\n",
        "    model = POSdir+'models/english-left3words-distsim.tagger'\n",
        "    pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n",
        "\n",
        "    pos_tag_stf = []\n",
        "\n",
        "    for _, tag in pos_tagger.tag(token):\n",
        "      if tag not in tag_to_num:\n",
        "        pos_tag_stf.append(-1)\n",
        "      else:\n",
        "        pos_tag_stf.append(tag_to_num[tag])\n",
        "          \n",
        "      # write word index and tag in train_X and train_X_tag\n",
        "    for wx, word in enumerate(token):\n",
        "        train_X_tag[sx, wx] = pos_tag_stf[wx]\n",
        "\n",
        "  return train_X_tag\n",
        "\n",
        "fn = data_dir + 'Restaurants_Train_v2.xml'\n",
        "X_train_tag_res = create_pos_tag(fn, POSdir, sent_len=100, sent_num=3050)"
      ],
      "metadata": {
        "id": "oTXp8EkJW6Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "_qRVPQShrQVB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "print(device)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 8\n",
        "\n",
        "NUM_ASPECT_TAGS = 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1cRHlwwrU6A",
        "outputId": "478d0805-6834-4b80-ec54-5c1a2b9876f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(pred, true):\n",
        "    pred = pred.view(-1, 3)\n",
        "    #print(pred.shape)\n",
        "    true = true.view(-1)\n",
        "    #print(true.shape)\n",
        "    loss = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(pred, dim = 1 ), true)\n",
        "    return loss\n",
        "\n",
        "def cal_acc(pred_tags, true_tags):\n",
        "    \n",
        "    batch = pred_tags.shape[0]\n",
        "    acc = 0\n",
        "\n",
        "    for i in range(batch):\n",
        "        pred_array = pred_tags[i].cpu().detach().numpy()\n",
        "        true_array = true_tags[i].cpu().detach().numpy()\n",
        "\n",
        "        f1_scores_class = f1_score(true_array, pred_array, labels = np.unique(true_array), average='weighted')\n",
        "        acc += np.sum(pred_array == true_array) / len(pred_array)\n",
        "\n",
        "    return acc / batch, f1_scores_class\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight=torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 5, padding=2 )\n",
        "        self.conv2=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 3, padding=1 )\n",
        "        self.dropout=torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size = 128, num_layers= 1, bidirectional= True, batch_first=True)\n",
        "\n",
        "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
        "  \n",
        "    def forward(self, x_train, y_train):\n",
        "    \n",
        "        x_emb=torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train) ), dim=2)\n",
        "\n",
        "        x_emb=self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv5(x_conv))\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "        \n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        x_result = torch.argmax(x_logit, dim =2)\n",
        "        loss = loss_fn(x_logit, y_train)\n",
        "\n",
        "        return x_result, loss"
      ],
      "metadata": {
        "id": "aY6ATEQATGVh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data_utils.TensorDataset(torch.Tensor(X_train_res), torch.Tensor(y_train_res))\n",
        "train_loader = data_utils.DataLoader(dataset, batch_size=35, shuffle=True)\n",
        "\n",
        "dataset_test = data_utils.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
        "test_loader = data_utils.DataLoader(dataset, batch_size=35, shuffle=True)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding), device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d3c2eb-3769-4736-9019-abb3f22550ee",
        "id": "TviNjs2RJxV9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (gen_embedding): Embedding(7878, 300)\n",
            "  (domain_embedding): Embedding(7878, 100)\n",
            "  (conv1): Conv1d(400, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv2): Conv1d(400, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_train_steps = int(len(X_train_res) / TRAIN_BATCH_SIZE * NUM_EPOCHS)\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer= AdamW(parameters, lr=3e-5)\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "  test_loss = []\n",
        "  test_acc = []\n",
        "  train_f1 = []\n",
        "  test_f1 = []\n",
        "\n",
        "  model.train()\n",
        "  for data in tqdm(train_loader, total=len(train_loader)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred_tags, loss = model(data[0].long(), data[1].long(), testing = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    true_tags = data[1].long()\n",
        "\n",
        "    acc, f1_score_value = cal_acc(pred_tags, true_tags)\n",
        "\n",
        "    train_acc.append(acc)\n",
        "    train_f1.append(f1_score_value)\n",
        "\n",
        "  avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "\n",
        "  avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "  avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for data in tqdm(test_loader, total=len(test_loader)):\n",
        "        \n",
        "    pred_tags, loss = model(data[0].long(), data[1].long(), testing = True)\n",
        "\n",
        "    test_loss.append(loss.item())\n",
        "    true_tags = data[1].long()\n",
        "\n",
        "    acc, f1_score_value = cal_acc(pred_tags, true_tags)\n",
        "\n",
        "    test_acc.append(acc)\n",
        "    test_f1.append(f1_score_value)\n",
        "\n",
        "  avg_test_f1 = sum(test_f1) / len(test_f1)\n",
        "  avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "  avg_test_acc = sum(test_acc) / len(test_acc)\n",
        "        \n",
        "  print(\"Train acc: {:.2f}%; Test acc: {:.2f}%\".format(avg_train_acc*100, avg_test_acc*100))\n",
        "  print(\"Train Loss: {:.5f}; Test Loss: {:.5f}\".format(avg_train_loss, avg_test_loss))\n",
        "  print(\"Train F1 score: {:.5f}; Test F1 score: {:.5f}\".format(avg_train_f1, avg_test_f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff90c364-2a1b-474c-e10a-e64cd032b3f1",
        "id": "fkCdTUvlRCOC"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 88/88 [01:12<00:00,  1.22it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.61%; Test acc: 98.61%\n",
            "Train Loss: 0.75179; Test Loss: 0.13065\n",
            "Train F1 score: 0.97747; Test F1 score: 0.97527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:12<00:00,  1.21it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.61%; Test acc: 98.60%\n",
            "Train Loss: 0.06993; Test Loss: 0.05725\n",
            "Train F1 score: 0.97932; Test F1 score: 0.97789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:10<00:00,  1.24it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.61%; Test acc: 98.61%\n",
            "Train Loss: 0.05520; Test Loss: 0.05220\n",
            "Train F1 score: 0.97997; Test F1 score: 0.98167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:11<00:00,  1.24it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.61%; Test acc: 98.61%\n",
            "Train Loss: 0.05137; Test Loss: 0.04878\n",
            "Train F1 score: 0.97543; Test F1 score: 0.97831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:11<00:00,  1.23it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.59%; Test acc: 98.61%\n",
            "Train Loss: 0.04914; Test Loss: 0.04637\n",
            "Train F1 score: 0.97611; Test F1 score: 0.97721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}