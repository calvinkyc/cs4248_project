{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q2Pp-VjL_5Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#data_dir =  '/content/drive/MyDrive/Colab Notebooks/'\n",
        "data_dir = './data/'\n",
        "\n",
        "#file to download to run model:  \n",
        "#1) https://howardhsu.github.io/dataset/ for domain embedding (need to download this!)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ],
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate word_index list\n",
        "def build_vocab(data_dir, plain = []):\n",
        "    \"\"\"plain is a empty str file which will record all text from official dataset\"\"\"\n",
        "    for fn in os.listdir(data_dir):\n",
        "        if fn.endswith('.xml'):\n",
        "            with open(data_dir+fn) as f:\n",
        "                dom=ET.parse(f)\n",
        "                root=dom.getroot()\n",
        "                for sent in root.iter(\"sentence\"):\n",
        "                    text = sent.find('text').text.lower()\n",
        "                    token = word_tokenize(text)\n",
        "                    plain = plain + token\n",
        "    vocab = sorted(set(plain))\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(vocab):\n",
        "        word_idx[word] = idx+1   \n",
        "    return word_idx\n",
        "\n",
        "word_indx = build_vocab(data_dir)"
      ],
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_indx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJVv6N0pTIR6",
        "outputId": "52c7ac18-8481-4580-9fdd-3c7a625b5bb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7868"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn+\".bin\")\n",
        "    embedding=np.zeros((len(word_idx)+2, dim) )\n",
        "\n",
        "    with open(fn) as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec=l.rstrip().split(' ')\n",
        "            if len(rec)==2: #skip the first line.\n",
        "                continue \n",
        "            # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:] ])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w] ].sum()==0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w] ] = model.get_word_vector(w)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "LiqrfwzMf50x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox9AKoXxeuG_",
        "outputId": "3886efe2-1443-41fa-aa9e-251a694ac3b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('glove.6B.zip', <http.client.HTTPMessage at 0x7f22a9389dd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/glove.6B.zip\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0wQZJwrezwt",
        "outputId": "cbb13094-92ed-42c8-9b1a-4aed412af8a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: /content/glove.6B.50d.txt  \n",
            "  inflating: /content/glove.6B.100d.txt  \n",
            "  inflating: /content/glove.6B.200d.txt  \n",
            "  inflating: /content/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fn = data_dir + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim = 100, emb = True)\n",
        "\n",
        "#fn = data_dir + 'laptop_emb.vec'\n",
        "#lap_domain_embedding = gen_np_embedding(fn, word_indx, dim = 100, emb = True)\n",
        "\n",
        "#fn = data_dir + 'glove.840B.300d.txt'\n",
        "fn = '/content/glove.6B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim = 300, emb = False)"
      ],
      "metadata": {
        "id": "PnbH1kXllslz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "general_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXde5DS5mjzC",
        "outputId": "d244a513-e4fc-4d70-c120-2cc48ae56f77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7870, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_data_restaurant(fn, word_idx,  sent_len=83, sent_num=3050):\n",
        "\n",
        "    corpus = []\n",
        "    corpus_tag = []\n",
        "    opsList = []\n",
        "    train_X = np.zeros((sent_num, sent_len), np.int16)\n",
        "\n",
        "    train_y = np.zeros((sent_num, sent_len), np.int16) \n",
        "\n",
        "    dom=ET.parse(fn)\n",
        "    root=dom.getroot()\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(root.iter(\"sentence\") ) : \n",
        "        text = sent.find('text').text.lower()\n",
        "        # tokenize the current sentence\n",
        "        token = word_tokenize(text)\n",
        "        corpus.append(token)\n",
        "            \n",
        "        # write word index and tag in train_X \n",
        "        for wx, word in enumerate(token):\n",
        "            train_X[sx, wx] = word_idx[word]\n",
        "\n",
        "        for ox, apin in enumerate(sent.iter('aspectTerms') ) :\n",
        "            for ax, opin in enumerate(apin.iter('aspectTerm')):\n",
        "                target, polarity, start, end = opin.attrib['term'], opin.attrib['polarity'], int(opin.attrib['from']), int(opin.attrib['to'])\n",
        "                # find word index (instead of str index) if start,end is not (0,0)\n",
        "                if end != 0:\n",
        "                    if start != 0:\n",
        "                        start = len(word_tokenize(text[:start]))\n",
        "                    end = len(word_tokenize(text[:end]))-1\n",
        "                    # for training only identify aspect word, but not polarity\n",
        "                    train_y[sx, start] = 1\n",
        "                    if end > start:\n",
        "                        train_y[sx, start+1:end] = 2   \n",
        "    return train_X, train_y\n",
        "\n",
        "fn = data_dir + 'Restaurants_Train_v2.xml'\n",
        "X_train_res, y_train_res = create_train_data_restaurant(fn, word_indx,sent_len=100)\n",
        "#fn = data_dir + 'Laptop_Train_v2.xml'\n",
        "#X_train_lap, y_train_lap = create_train_data_restaurant(fn, word_indx,sent_len=100)\n",
        "fn_test = data_dir + 'Restaurants_Test_Gold.xml'\n",
        "X_test, y_test = create_train_data_restaurant(fn_test, word_indx, sent_len=100)"
      ],
      "metadata": {
        "id": "bITAH2M_J-Js"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4zg5HBqgLZU",
        "outputId": "a4161395-f835-42c7-de7f-0dd53dc0e0bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1187, 6978, 6583, ...,    0,    0,    0],\n",
              "       [7072,  876, 1623, ...,    0,    0,    0],\n",
              "       [6978, 2857, 3741, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    0,    0,    0],\n",
              "       [   0,    0,    0, ...,    0,    0,    0],\n",
              "       [   0,    0,    0, ...,    0,    0,    0]], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(filter(lambda x: x[1] == 6978, word_indx.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb34f8e-f947-4c7c-9362-e7664349b35e",
        "id": "wOdt3SjVg6fo"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 6978)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(filter(lambda x: x[1] == 6583, word_indx.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBcecXVrgjWY",
        "outputId": "d18d08f4-b871-421e-fdf5-e64988a4a6d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('staff', 6583)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_res[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JLhLsakgRCO",
        "outputId": "186c85cc-f00e-4bb3-e450-05dc1f7f4808"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1187, 6978, 6583, 7597, 6417, 3436, 7072, 7399,   74,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UVvu_VtgO-t",
        "outputId": "6cfb9057-c296-4c36-e38e-9469a3c2b76e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_res[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWtdsjrCgUDM",
        "outputId": "84e03d56-85ff-43e4-f918-223e59b93baa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not used - try to add a POS tag layer\n",
        "\n",
        "#pos_tag_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP','SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',',','.',':','$','#',\"``\",\"''\",'(',')','-LRB-','-RRB-','HYPH','NFP','SYM','PUNC','AFX','ADD']\n",
        "#pos_len = 53\n",
        "\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "POSdir = data_dir + 'stanford-postagger-full/'\n",
        "\n",
        "def create_pos_tag(fn, POSdir, sent_len=83, sent_num=3050):\n",
        "    pos_tag_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS','NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP','SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',',','.',':','$','#',\"``\",\"''\",'(',')','-LRB-','-RRB-','HYPH','NFP','SYM','PUNC','AFX','ADD']\n",
        "    tag_to_num = {tag:i+1 for i, tag in enumerate(sorted(pos_tag_list))}\n",
        "\n",
        "    corpus_tag = []\n",
        "    \n",
        "    train_X_tag = np.zeros((sent_num, sent_len), np.int16)\n",
        "    \n",
        "    dom=ET.parse(fn)\n",
        "    root=dom.getroot()\n",
        "\n",
        "    for sx, sent in enumerate(root.iter(\"sentence\") ) : \n",
        "        text = sent.find('text').text\n",
        "        token = word_tokenize(text)\n",
        "\n",
        "        jar = POSdir+'stanford-postagger.jar'\n",
        "        model = POSdir+'models/english-left3words-distsim.tagger'\n",
        "        pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n",
        "\n",
        "        pos_tag_stf = []\n",
        "\n",
        "        for _, tag in pos_tagger.tag(token):\n",
        "            if tag not in tag_to_num:\n",
        "                pos_tag_stf.append(-1)\n",
        "            else:\n",
        "                pos_tag_stf.append(tag_to_num[tag])\n",
        "                \n",
        "          # write word index and tag in train_X and train_X_tag\n",
        "        for wx, word in enumerate(token):\n",
        "            train_X_tag[sx, wx] = pos_tag_stf[wx]\n",
        "    return train_X_tag\n",
        "\n",
        "fn = data_dir + 'Restaurants_Train_v2.xml'\n",
        "X_train_tag_res = create_pos_tag(fn, POSdir, sent_len=100, sent_num=3050)"
      ],
      "metadata": {
        "id": "oTXp8EkJW6Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "  \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "_qRVPQShrQVB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "print(device)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 8\n",
        "\n",
        "NUM_ASPECT_TAGS = 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1cRHlwwrU6A",
        "outputId": "a33e8f9a-a6af-47c8-e1aa-e69ffde4151d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(pred, true):\n",
        "    pred = pred.view(-1, 3)\n",
        "    #print(pred.shape)\n",
        "    true = true.view(-1)\n",
        "    #print(true.shape)\n",
        "    loss = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(pred, dim = 1 ), true)\n",
        "    return loss\n",
        "\n",
        "def cal_acc(pred_tags, true_tags):\n",
        "    \n",
        "    batch = pred_tags.shape[0]\n",
        "    acc = 0\n",
        "\n",
        "    for i in range(batch):\n",
        "        pred_array = pred_tags[i].cpu().detach().numpy()\n",
        "        true_array = true_tags[i].cpu().detach().numpy()\n",
        "\n",
        "        f1_scores_class = f1_score(true_array, pred_array, labels = np.unique(true_array), average='weighted')\n",
        "        acc += np.sum(pred_array == true_array) / len(pred_array)\n",
        "\n",
        "    return acc / batch, f1_scores_class\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight=torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 5, padding=2 )\n",
        "        self.conv2=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 3, padding=1 )\n",
        "        self.dropout=torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size = 128, num_layers= 1, bidirectional= True, batch_first=True)\n",
        "\n",
        "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
        "  \n",
        "    def forward(self, x_train, y_train):\n",
        "        x_emb=torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train) ), dim=2)\n",
        "        x_emb=self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "        \n",
        "        x_conv = torch.nn.functional.relu(self.conv5(x_conv))\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "        \n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        x_result = torch.argmax(x_logit, dim =2)\n",
        "        loss = loss_fn(x_logit, y_train)\n",
        "\n",
        "        return x_result, loss"
      ],
      "metadata": {
        "id": "aY6ATEQATGVh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(torch.Tensor(X_train_res), torch.Tensor(y_train_res))\n",
        "train_loader = DataLoader(dataset, batch_size=35, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
        "test_loader = DataLoader(dataset, batch_size=35, shuffle=True)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding), device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e93146-6aa3-41ac-e422-3eee95ed2fa5",
        "id": "TviNjs2RJxV9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (gen_embedding): Embedding(7870, 300)\n",
            "  (domain_embedding): Embedding(7870, 100)\n",
            "  (conv1): Conv1d(400, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv2): Conv1d(400, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_train_steps = int(len(X_train_res) / TRAIN_BATCH_SIZE * NUM_EPOCHS)\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer= AdamW(parameters, lr=3e-5)\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_tags, loss = model(data[0].long(), data[1].long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        true_tags = data[1].long()\n",
        "\n",
        "        acc, f1_score_value = cal_acc(pred_tags, true_tags)\n",
        "\n",
        "        train_acc.append(acc)\n",
        "        train_f1.append(f1_score_value)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data in tqdm(test_loader, total=len(test_loader)):\n",
        "          \n",
        "        pred_tags, loss = model(data[0].long(), data[1].long())\n",
        "\n",
        "        test_loss.append(loss.item())\n",
        "        true_tags = data[1].long()\n",
        "\n",
        "        acc, f1_score_value = cal_acc(pred_tags, true_tags)\n",
        "\n",
        "        test_acc.append(acc)\n",
        "        test_f1.append(f1_score_value)\n",
        "\n",
        "    avg_test_f1 = sum(test_f1) / len(test_f1)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "    avg_test_acc = sum(test_acc) / len(test_acc)\n",
        "          \n",
        "    print(\"Train acc: {:.2f}%; Test acc: {:.2f}%\".format(avg_train_acc*100, avg_test_acc*100))\n",
        "    print(\"Train Loss: {:.5f}; Test Loss: {:.5f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"Train F1 score: {:.5f}; Test F1 score: {:.5f}\".format(avg_train_f1, avg_test_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92495fe-b313-46b7-faa3-fb9961944e61",
        "id": "fkCdTUvlRCOC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 88/88 [01:13<00:00,  1.20it/s]\n",
            "100%|██████████| 88/88 [00:28<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 75.90%; Test acc: 98.61%\n",
            "Train Loss: 0.83030; Test Loss: 0.18817\n",
            "Train F1 score: 0.77289; Test F1 score: 0.97755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:13<00:00,  1.19it/s]\n",
            "100%|██████████| 88/88 [00:28<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.61%; Test acc: 98.61%\n",
            "Train Loss: 0.08027; Test Loss: 0.05915\n",
            "Train F1 score: 0.97219; Test F1 score: 0.98201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:13<00:00,  1.20it/s]\n",
            "100%|██████████| 88/88 [00:28<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.59%; Test acc: 98.60%\n",
            "Train Loss: 0.05743; Test Loss: 0.05435\n",
            "Train F1 score: 0.98049; Test F1 score: 0.97917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:13<00:00,  1.20it/s]\n",
            "100%|██████████| 88/88 [00:28<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.60%; Test acc: 98.61%\n",
            "Train Loss: 0.05336; Test Loss: 0.05075\n",
            "Train F1 score: 0.97746; Test F1 score: 0.97574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [01:13<00:00,  1.20it/s]\n",
            "100%|██████████| 88/88 [00:27<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 98.60%; Test acc: 98.61%\n",
            "Train Loss: 0.05078; Test Loss: 0.04822\n",
            "Train F1 score: 0.97863; Test F1 score: 0.97835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}