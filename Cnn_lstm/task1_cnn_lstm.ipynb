{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model.bin\"\n",
        "\n",
        "#file to download to run model:  \n",
        "#1) https://howardhsu.github.io/dataset/ for domain embedding (need to download this!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "outputs": [],
      "source": [
        "from ast import FloorDiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3432\n",
            "3432\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, aspect_tags, sent_len=83):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "                train_y[sx, wx] = aspect_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 2)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        return x_logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "fn = DATA_DIR + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "res_domain_embedding = np.concatenate([res_domain_embedding, lap_domain_embedding], axis=0)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, aspect_tags, sent_len=83)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2745\n",
            "valid samples:687\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 2\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=2), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:0.160 valid_loss:0.181\n",
            "\ttrain_acc:93.76% valid_acc:92.79%\n",
            "\ttrain_f1:0.937 valid_f1:0.928\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5100  1512]\n",
            " [ 1187 35432]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1328  427]\n",
            " [ 386 9128]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:0.158 valid_loss:0.182\n",
            "\ttrain_acc:93.82% valid_acc:92.83%\n",
            "\ttrain_f1:0.938 valid_f1:0.927\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5134  1491]\n",
            " [ 1180 35430]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1301  454]\n",
            " [ 354 9160]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.96it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:0.156 valid_loss:0.184\n",
            "\ttrain_acc:93.88% valid_acc:92.55%\n",
            "\ttrain_f1:0.938 valid_f1:0.924\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5126  1505]\n",
            " [ 1136 35417]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1272  483]\n",
            " [ 357 9157]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.89it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.151 valid_loss:0.183\n",
            "\ttrain_acc:94.06% valid_acc:92.80%\n",
            "\ttrain_f1:0.940 valid_f1:0.927\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5204  1428]\n",
            " [ 1141 35473]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1295  460]\n",
            " [ 351 9163]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.147 valid_loss:0.183\n",
            "\ttrain_acc:94.10% valid_acc:92.93%\n",
            "\ttrain_f1:0.940 valid_f1:0.929\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5164  1479]\n",
            " [ 1076 35593]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1311  444]\n",
            " [ 353 9161]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.147 valid_loss:0.177\n",
            "\ttrain_acc:94.30% valid_acc:93.23%\n",
            "\ttrain_f1:0.943 valid_f1:0.931\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5276  1354]\n",
            " [ 1109 35469]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1303  452]\n",
            " [ 311 9203]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.87it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.145 valid_loss:0.173\n",
            "\ttrain_acc:94.30% valid_acc:93.40%\n",
            "\ttrain_f1:0.942 valid_f1:0.934\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5241  1418]\n",
            " [ 1051 35619]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1358  397]\n",
            " [ 347 9167]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.85it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.143 valid_loss:0.172\n",
            "\ttrain_acc:94.47% valid_acc:93.33%\n",
            "\ttrain_f1:0.944 valid_f1:0.933\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5274  1358]\n",
            " [ 1036 35626]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1335  420]\n",
            " [ 332 9182]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  2.02it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.140 valid_loss:0.181\n",
            "\ttrain_acc:94.65% valid_acc:92.95%\n",
            "\ttrain_f1:0.946 valid_f1:0.929\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5325  1302]\n",
            " [ 1011 35629]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1345  410]\n",
            " [ 385 9129]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.138 valid_loss:0.176\n",
            "\ttrain_acc:94.60% valid_acc:93.11%\n",
            "\ttrain_f1:0.945 valid_f1:0.930\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5316  1328]\n",
            " [ 1010 35618]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1281  474]\n",
            " [ 302 9212]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.139 valid_loss:0.167\n",
            "\ttrain_acc:94.63% valid_acc:93.39%\n",
            "\ttrain_f1:0.946 valid_f1:0.933\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5347  1307]\n",
            " [ 1023 35693]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1323  432]\n",
            " [ 313 9201]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.133 valid_loss:0.174\n",
            "\ttrain_acc:94.88% valid_acc:93.17%\n",
            "\ttrain_f1:0.948 valid_f1:0.931\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5362  1293]\n",
            " [  926 35744]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1314  441]\n",
            " [ 329 9185]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.91it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.129 valid_loss:0.166\n",
            "\ttrain_acc:95.07% valid_acc:93.20%\n",
            "\ttrain_f1:0.950 valid_f1:0.932\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5422  1191]\n",
            " [  942 35705]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1364  391]\n",
            " [ 375 9139]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.129 valid_loss:0.172\n",
            "\ttrain_acc:94.97% valid_acc:93.30%\n",
            "\ttrain_f1:0.949 valid_f1:0.932\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5434  1208]\n",
            " [  963 35596]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1318  437]\n",
            " [ 318 9196]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.127 valid_loss:0.168\n",
            "\ttrain_acc:95.04% valid_acc:93.50%\n",
            "\ttrain_f1:0.950 valid_f1:0.934\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5450  1210]\n",
            " [  940 35771]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1311  444]\n",
            " [ 288 9226]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.94it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.124 valid_loss:0.165\n",
            "\ttrain_acc:95.21% valid_acc:93.84%\n",
            "\ttrain_f1:0.952 valid_f1:0.938\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5411  1225]\n",
            " [  849 35795]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1357  398]\n",
            " [ 296 9218]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.97it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.123 valid_loss:0.169\n",
            "\ttrain_acc:95.29% valid_acc:93.41%\n",
            "\ttrain_f1:0.953 valid_f1:0.933\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5523  1128]\n",
            " [  907 35609]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1322  433]\n",
            " [ 310 9204]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.122 valid_loss:0.158\n",
            "\ttrain_acc:95.29% valid_acc:93.85%\n",
            "\ttrain_f1:0.952 valid_f1:0.938\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5482  1159]\n",
            " [  881 35778]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1386  369]\n",
            " [ 324 9190]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.121 valid_loss:0.170\n",
            "\ttrain_acc:95.34% valid_acc:93.51%\n",
            "\ttrain_f1:0.953 valid_f1:0.935\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5497  1148]\n",
            " [  868 35765]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1361  394]\n",
            " [ 337 9177]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.121 valid_loss:0.167\n",
            "\ttrain_acc:95.43% valid_acc:93.69%\n",
            "\ttrain_f1:0.954 valid_f1:0.936\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5526  1118]\n",
            " [  862 35799]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1326  429]\n",
            " [ 282 9232]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"aspact_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"aspact_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['aspact_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['aspact_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.85, 1.0)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['aspact_train_acc'], label='train accuracy')\n",
        "plt.plot(history['aspact_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.85, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AT       0.79      0.90      0.84      1755\n",
            "         NAT       0.98      0.96      0.97      9514\n",
            "\n",
            "    accuracy                           0.95     11269\n",
            "   macro avg       0.89      0.93      0.90     11269\n",
            "weighted avg       0.95      0.95      0.95     11269\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_aspect_tags = []\n",
        "    final_true_aspect_tags = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_aspect_tags.extend(pred_tags)\n",
        "            final_true_aspect_tags.extend(label)\n",
        "\n",
        "    final_pred_aspect_tags = torch.stack(final_pred_aspect_tags).cpu()\n",
        "    final_true_aspect_tags = torch.stack(final_true_aspect_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_aspect_tags, final_pred_aspect_tags, \n",
        "                                target_names=encoder.classes_))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
