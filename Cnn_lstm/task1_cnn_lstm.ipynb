{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model.bin\"\n",
        "\n",
        "#file to download to run model:  \n",
        "#1) https://howardhsu.github.io/dataset/ for domain embedding (need to download this!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "outputs": [],
      "source": [
        "from ast import FloorDiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## No Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, aspect_tags, sent_len=83):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "                train_y[sx, wx] = aspect_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 2)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        out = torch.nn.functional.softmax(x_logit)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "fn = DATA_DIR + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "res_domain_embedding = np.concatenate([res_domain_embedding, lap_domain_embedding], axis=0)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, aspect_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 2\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=2), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.94it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:0.693 valid_loss:0.693\n",
            "\ttrain_acc:68.51% valid_acc:72.53%\n",
            "\ttrain_f1:0.721 valid_f1:0.748\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1981  4830]\n",
            " [10810 32046]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 484 1279]\n",
            " [2239 8805]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:0.690 valid_loss:0.692\n",
            "\ttrain_acc:73.80% valid_acc:74.09%\n",
            "\ttrain_f1:0.757 valid_f1:0.758\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1801  4989]\n",
            " [ 8065 34968]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 447 1316]\n",
            " [2002 9042]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:0.687 valid_loss:0.691\n",
            "\ttrain_acc:76.04% valid_acc:76.53%\n",
            "\ttrain_f1:0.773 valid_f1:0.780\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1884  4905]\n",
            " [ 6969 35800]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 573 1190]\n",
            " [1816 9228]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.684 valid_loss:0.691\n",
            "\ttrain_acc:77.48% valid_acc:77.71%\n",
            "\ttrain_f1:0.789 valid_f1:0.792\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2516  4280]\n",
            " [ 6904 35954]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 680 1083]\n",
            " [1772 9272]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.684 valid_loss:0.691\n",
            "\ttrain_acc:79.65% valid_acc:80.38%\n",
            "\ttrain_f1:0.806 valid_f1:0.811\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2562  4204]\n",
            " [ 5914 37045]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 679 1084]\n",
            " [1429 9615]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.683 valid_loss:0.690\n",
            "\ttrain_acc:80.89% valid_acc:82.13%\n",
            "\ttrain_f1:0.818 valid_f1:0.825\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2910  3883]\n",
            " [ 5604 37243]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 700 1063]\n",
            " [1226 9818]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.682 valid_loss:0.690\n",
            "\ttrain_acc:82.34% valid_acc:83.24%\n",
            "\ttrain_f1:0.829 valid_f1:0.836\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2980  3791]\n",
            " [ 4934 37703]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 781  982]\n",
            " [1164 9880]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.682 valid_loss:0.690\n",
            "\ttrain_acc:83.06% valid_acc:83.58%\n",
            "\ttrain_f1:0.838 valid_f1:0.841\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3438  3348]\n",
            " [ 5059 37779]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 850  913]\n",
            " [1190 9854]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.681 valid_loss:0.690\n",
            "\ttrain_acc:84.29% valid_acc:83.48%\n",
            "\ttrain_f1:0.848 valid_f1:0.843\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3528  3274]\n",
            " [ 4533 38359]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 964  799]\n",
            " [1317 9727]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.681 valid_loss:0.690\n",
            "\ttrain_acc:84.68% valid_acc:83.85%\n",
            "\ttrain_f1:0.853 valid_f1:0.849\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3762  3023]\n",
            " [ 4574 38226]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1086  677]\n",
            " [1391 9653]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.97it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.679 valid_loss:0.690\n",
            "\ttrain_acc:85.58% valid_acc:85.02%\n",
            "\ttrain_f1:0.862 valid_f1:0.859\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3974  2818]\n",
            " [ 4332 38461]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1094  669]\n",
            " [1250 9794]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.87it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.679 valid_loss:0.690\n",
            "\ttrain_acc:86.25% valid_acc:86.73%\n",
            "\ttrain_f1:0.869 valid_f1:0.872\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4209  2577]\n",
            " [ 4259 38677]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1059   704]\n",
            " [  995 10049]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.678 valid_loss:0.689\n",
            "\ttrain_acc:87.37% valid_acc:87.62%\n",
            "\ttrain_f1:0.878 valid_f1:0.881\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4217  2564]\n",
            " [ 3705 39135]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1153   610]\n",
            " [  975 10069]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.678 valid_loss:0.689\n",
            "\ttrain_acc:88.27% valid_acc:89.13%\n",
            "\ttrain_f1:0.887 valid_f1:0.892\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4574  2218]\n",
            " [ 3582 39059]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1104   659]\n",
            " [  733 10311]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.678 valid_loss:0.689\n",
            "\ttrain_acc:89.56% valid_acc:88.99%\n",
            "\ttrain_f1:0.897 valid_f1:0.892\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4365  2382]\n",
            " [ 2792 40027]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1149   614]\n",
            " [  796 10248]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:89.34% valid_acc:90.36%\n",
            "\ttrain_f1:0.896 valid_f1:0.901\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4482  2308]\n",
            " [ 2995 39946]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1044   719]\n",
            " [  515 10529]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:90.17% valid_acc:90.54%\n",
            "\ttrain_f1:0.903 valid_f1:0.904\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4538  2270]\n",
            " [ 2624 40346]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1101   662]\n",
            " [  550 10494]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:90.43% valid_acc:91.31%\n",
            "\ttrain_f1:0.905 valid_f1:0.909\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4483  2304]\n",
            " [ 2456 40492]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1033   730]\n",
            " [  383 10661]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:90.85% valid_acc:91.05%\n",
            "\ttrain_f1:0.908 valid_f1:0.909\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4494  2303]\n",
            " [ 2243 40645]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1134   629]\n",
            " [  517 10527]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:91.08% valid_acc:90.89%\n",
            "\ttrain_f1:0.910 valid_f1:0.909\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4375  2414]\n",
            " [ 2007 40792]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1164   599]\n",
            " [  568 10476]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"aspact_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"aspact_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['aspact_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['aspact_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5, 1.0)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['aspact_train_acc'], label='train accuracy')\n",
        "plt.plot(history['aspact_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.5, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AT       0.76      0.65      0.70      1763\n",
            "         NAT       0.95      0.97      0.96     11044\n",
            "\n",
            "    accuracy                           0.92     12807\n",
            "   macro avg       0.85      0.81      0.83     12807\n",
            "weighted avg       0.92      0.92      0.92     12807\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_aspect_tags = []\n",
        "    final_true_aspect_tags = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_aspect_tags.extend(pred_tags)\n",
        "            final_true_aspect_tags.extend(label)\n",
        "\n",
        "    final_pred_aspect_tags = torch.stack(final_pred_aspect_tags).cpu()\n",
        "    final_true_aspect_tags = torch.stack(final_true_aspect_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_aspect_tags, final_pred_aspect_tags, \n",
        "                                target_names=encoder.classes_))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task1_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, aspect_tags, sent_len=83):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "                train_y[sx, wx] = aspect_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 2)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        out = torch.nn.functional.softmax(x_logit)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "fn = DATA_DIR + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "res_domain_embedding = np.concatenate([res_domain_embedding, lap_domain_embedding], axis=0)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, aspect_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 2\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=2), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:0.693 valid_loss:0.693\n",
            "\ttrain_acc:69.53% valid_acc:75.92%\n",
            "\ttrain_f1:0.719 valid_f1:0.765\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1973  4770]\n",
            " [ 8356 27986]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 458 1262]\n",
            " [1486 8208]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:0.689 valid_loss:0.692\n",
            "\ttrain_acc:75.55% valid_acc:77.19%\n",
            "\ttrain_f1:0.761 valid_f1:0.775\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1802  4888]\n",
            " [ 5705 30937]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 481 1239]\n",
            " [1364 8330]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:0.686 valid_loss:0.691\n",
            "\ttrain_acc:77.93% valid_acc:79.00%\n",
            "\ttrain_f1:0.782 valid_f1:0.793\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2161  4554]\n",
            " [ 4983 31522]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 590 1130]\n",
            " [1267 8427]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.684 valid_loss:0.691\n",
            "\ttrain_acc:78.53% valid_acc:80.59%\n",
            "\ttrain_f1:0.793 valid_f1:0.811\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2740  3980]\n",
            " [ 5345 31359]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 729  991]\n",
            " [1225 8469]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.683 valid_loss:0.691\n",
            "\ttrain_acc:80.83% valid_acc:81.90%\n",
            "\ttrain_f1:0.812 valid_f1:0.824\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2918  3811]\n",
            " [ 4534 32267]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 812  908]\n",
            " [1158 8536]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.682 valid_loss:0.691\n",
            "\ttrain_acc:81.75% valid_acc:83.22%\n",
            "\ttrain_f1:0.823 valid_f1:0.835\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3311  3415]\n",
            " [ 4517 32222]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 829  891]\n",
            " [1024 8670]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.87it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.682 valid_loss:0.691\n",
            "\ttrain_acc:82.70% valid_acc:83.66%\n",
            "\ttrain_f1:0.832 valid_f1:0.840\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3454  3277]\n",
            " [ 4225 32416]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 877  843]\n",
            " [1022 8672]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.89it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.681 valid_loss:0.690\n",
            "\ttrain_acc:83.69% valid_acc:84.37%\n",
            "\ttrain_f1:0.842 valid_f1:0.849\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3718  3016]\n",
            " [ 4068 32621]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 973  747]\n",
            " [1037 8657]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.680 valid_loss:0.690\n",
            "\ttrain_acc:84.14% valid_acc:85.37%\n",
            "\ttrain_f1:0.847 valid_f1:0.853\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3913  2842]\n",
            " [ 4041 32611]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 874  846]\n",
            " [ 824 8870]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.680 valid_loss:0.690\n",
            "\ttrain_acc:85.26% valid_acc:85.38%\n",
            "\ttrain_f1:0.856 valid_f1:0.858\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3946  2772]\n",
            " [ 3610 32959]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1011  709]\n",
            " [ 960 8734]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.679 valid_loss:0.690\n",
            "\ttrain_acc:85.88% valid_acc:87.17%\n",
            "\ttrain_f1:0.862 valid_f1:0.870\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4092  2633]\n",
            " [ 3478 33091]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 931  789]\n",
            " [ 675 9019]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.679 valid_loss:0.690\n",
            "\ttrain_acc:86.36% valid_acc:87.02%\n",
            "\ttrain_f1:0.867 valid_f1:0.872\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4270  2466]\n",
            " [ 3442 33136]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1027  693]\n",
            " [ 788 8906]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.678 valid_loss:0.690\n",
            "\ttrain_acc:87.28% valid_acc:87.52%\n",
            "\ttrain_f1:0.875 valid_f1:0.876\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4312  2395]\n",
            " [ 3117 33520]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1049  671]\n",
            " [ 753 8941]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.92it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.678 valid_loss:0.690\n",
            "\ttrain_acc:87.65% valid_acc:88.23%\n",
            "\ttrain_f1:0.879 valid_f1:0.882\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4428  2286]\n",
            " [ 3064 33536]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1048  672]\n",
            " [ 671 9023]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.677 valid_loss:0.690\n",
            "\ttrain_acc:88.36% valid_acc:88.22%\n",
            "\ttrain_f1:0.884 valid_f1:0.884\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4179  2506]\n",
            " [ 2551 34202]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1111  609]\n",
            " [ 735 8959]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.89it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.677 valid_loss:0.690\n",
            "\ttrain_acc:88.93% valid_acc:87.94%\n",
            "\ttrain_f1:0.891 valid_f1:0.884\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4510  2216]\n",
            " [ 2585 34076]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1194  526]\n",
            " [ 851 8843]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:89.30% valid_acc:89.20%\n",
            "\ttrain_f1:0.894 valid_f1:0.893\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4576  2157]\n",
            " [ 2479 34130]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1128  592]\n",
            " [ 641 9053]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:89.71% valid_acc:89.44%\n",
            "\ttrain_f1:0.897 valid_f1:0.896\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4472  2259]\n",
            " [ 2191 34317]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1163  557]\n",
            " [ 648 9046]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.91it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.677 valid_loss:0.690\n",
            "\ttrain_acc:90.32% valid_acc:89.57%\n",
            "\ttrain_f1:0.902 valid_f1:0.897\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4467  2249]\n",
            " [ 1954 34752]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1192  528]\n",
            " [ 663 9031]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.88it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.677 valid_loss:0.689\n",
            "\ttrain_acc:90.14% valid_acc:90.63%\n",
            "\ttrain_f1:0.901 valid_f1:0.904\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4502  2244]\n",
            " [ 2034 34618]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1103  617]\n",
            " [ 453 9241]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"aspact_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"aspact_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['aspact_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['aspact_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['aspact_train_acc'], label='train accuracy')\n",
        "plt.plot(history['aspact_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.65, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_15944/972914119.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.softmax(x_logit)\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AT       0.74      0.71      0.73      1720\n",
            "         NAT       0.95      0.96      0.95      9694\n",
            "\n",
            "    accuracy                           0.92     11414\n",
            "   macro avg       0.85      0.83      0.84     11414\n",
            "weighted avg       0.92      0.92      0.92     11414\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_aspect_tags = []\n",
        "    final_true_aspect_tags = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_aspect_tags.extend(pred_tags)\n",
        "            final_true_aspect_tags.extend(label)\n",
        "\n",
        "    final_pred_aspect_tags = torch.stack(final_pred_aspect_tags).cpu()\n",
        "    final_true_aspect_tags = torch.stack(final_true_aspect_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_aspect_tags, final_pred_aspect_tags, \n",
        "                                target_names=encoder.classes_))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
