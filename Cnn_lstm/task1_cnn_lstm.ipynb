{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model.bin\"\n",
        "\n",
        "#file to download to run model:  \n",
        "#1) https://howardhsu.github.io/dataset/ for domain embedding (need to download this!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "outputs": [],
      "source": [
        "from ast import FloorDiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## No Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, aspect_tags, sent_len=83):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "                train_y[sx, wx] = aspect_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 2)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        return x_logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "fn = DATA_DIR + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "res_domain_embedding = np.concatenate([res_domain_embedding, lap_domain_embedding], axis=0)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, aspect_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 2\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=2), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.90it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:0.517 valid_loss:0.421\n",
            "\ttrain_acc:85.11% valid_acc:86.12%\n",
            "\ttrain_f1:0.797 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[  113  6678]\n",
            " [  698 42063]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:0.403 valid_loss:0.403\n",
            "\ttrain_acc:86.41% valid_acc:86.12%\n",
            "\ttrain_f1:0.801 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6751]\n",
            " [    0 42926]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.81it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:0.396 valid_loss:0.393\n",
            "\ttrain_acc:86.30% valid_acc:86.12%\n",
            "\ttrain_f1:0.800 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6769]\n",
            " [    0 42645]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.84it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.382 valid_loss:0.376\n",
            "\ttrain_acc:86.41% valid_acc:86.12%\n",
            "\ttrain_f1:0.801 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6725]\n",
            " [    0 42749]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.83it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.364 valid_loss:0.353\n",
            "\ttrain_acc:86.38% valid_acc:86.12%\n",
            "\ttrain_f1:0.801 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6733]\n",
            " [    0 42704]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.83it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.331 valid_loss:0.314\n",
            "\ttrain_acc:86.33% valid_acc:86.12%\n",
            "\ttrain_f1:0.800 valid_f1:0.797\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6781]\n",
            " [    0 42827]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[    0  1794]\n",
            " [    0 11132]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.85it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.288 valid_loss:0.277\n",
            "\ttrain_acc:86.39% valid_acc:86.39%\n",
            "\ttrain_f1:0.802 valid_f1:0.810\n",
            "\ttrain_confusion_matrix:\n",
            "[[   37  6694]\n",
            " [   19 42570]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   86  1708]\n",
            " [   51 11081]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.84it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.255 valid_loss:0.242\n",
            "\ttrain_acc:88.50% valid_acc:89.79%\n",
            "\ttrain_f1:0.861 valid_f1:0.886\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 1786  5004]\n",
            " [  702 42121]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[  747  1047]\n",
            " [  273 10859]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.214 valid_loss:0.209\n",
            "\ttrain_acc:91.42% valid_acc:91.81%\n",
            "\ttrain_f1:0.909 valid_f1:0.916\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 3758  2987]\n",
            " [ 1262 41535]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1176   618]\n",
            " [  441 10691]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.193 valid_loss:0.198\n",
            "\ttrain_acc:92.39% valid_acc:92.33%\n",
            "\ttrain_f1:0.922 valid_f1:0.921\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4546  2209]\n",
            " [ 1560 41180]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1194   600]\n",
            " [  392 10740]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.184 valid_loss:0.191\n",
            "\ttrain_acc:92.71% valid_acc:92.51%\n",
            "\ttrain_f1:0.926 valid_f1:0.923\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4692  2069]\n",
            " [ 1539 41203]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1216   578]\n",
            " [  390 10742]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.84it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.176 valid_loss:0.185\n",
            "\ttrain_acc:93.07% valid_acc:92.29%\n",
            "\ttrain_f1:0.930 valid_f1:0.923\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4806  1962]\n",
            " [ 1465 41244]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1276   518]\n",
            " [  478 10654]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.78it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.171 valid_loss:0.182\n",
            "\ttrain_acc:93.24% valid_acc:92.99%\n",
            "\ttrain_f1:0.931 valid_f1:0.929\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4793  1941]\n",
            " [ 1409 41415]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1268   526]\n",
            " [  380 10752]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.83it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.168 valid_loss:0.181\n",
            "\ttrain_acc:93.38% valid_acc:92.77%\n",
            "\ttrain_f1:0.933 valid_f1:0.927\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4821  1898]\n",
            " [ 1382 41444]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1292   502]\n",
            " [  432 10700]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.84it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.163 valid_loss:0.177\n",
            "\ttrain_acc:93.55% valid_acc:92.90%\n",
            "\ttrain_f1:0.934 valid_f1:0.929\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4875  1873]\n",
            " [ 1322 41480]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1361   433]\n",
            " [  485 10647]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.162 valid_loss:0.172\n",
            "\ttrain_acc:93.59% valid_acc:93.25%\n",
            "\ttrain_f1:0.935 valid_f1:0.931\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4962  1798]\n",
            " [ 1368 41267]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1266   528]\n",
            " [  344 10788]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.159 valid_loss:0.173\n",
            "\ttrain_acc:93.69% valid_acc:93.26%\n",
            "\ttrain_f1:0.936 valid_f1:0.932\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4967  1793]\n",
            " [ 1327 41387]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1295   499]\n",
            " [  372 10760]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.155 valid_loss:0.168\n",
            "\ttrain_acc:93.86% valid_acc:93.41%\n",
            "\ttrain_f1:0.938 valid_f1:0.933\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5022  1770]\n",
            " [ 1274 41481]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1311   483]\n",
            " [  369 10763]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.152 valid_loss:0.169\n",
            "\ttrain_acc:94.09% valid_acc:93.53%\n",
            "\ttrain_f1:0.940 valid_f1:0.933\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5077  1674]\n",
            " [ 1247 41440]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1260   534]\n",
            " [  302 10830]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.83it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.149 valid_loss:0.159\n",
            "\ttrain_acc:94.12% valid_acc:93.73%\n",
            "\ttrain_f1:0.940 valid_f1:0.936\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5097  1686]\n",
            " [ 1230 41576]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 1317   477]\n",
            " [  334 10798]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"aspact_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"aspact_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['aspact_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['aspact_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8, 1.0)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['aspact_train_acc'], label='train accuracy')\n",
        "plt.plot(history['aspact_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.8, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AT       0.78      0.85      0.81      1794\n",
            "         NAT       0.98      0.96      0.97     11132\n",
            "\n",
            "    accuracy                           0.95     12926\n",
            "   macro avg       0.88      0.91      0.89     12926\n",
            "weighted avg       0.95      0.95      0.95     12926\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_aspect_tags = []\n",
        "    final_true_aspect_tags = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_aspect_tags.extend(pred_tags)\n",
        "            final_true_aspect_tags.extend(label)\n",
        "\n",
        "    final_pred_aspect_tags = torch.stack(final_pred_aspect_tags).cpu()\n",
        "    final_true_aspect_tags = torch.stack(final_true_aspect_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_aspect_tags, final_pred_aspect_tags, \n",
        "                                target_names=encoder.classes_))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task1_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, aspect_tags, sent_len=83):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "                train_y[sx, wx] = aspect_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 2)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        return x_logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "fn = DATA_DIR + 'laptop_emb.vec'\n",
        "lap_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "res_domain_embedding = np.concatenate([res_domain_embedding, lap_domain_embedding], axis=0)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, aspect_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 2\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=2), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  2.00it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:0.540 valid_loss:0.441\n",
            "\ttrain_acc:84.38% valid_acc:84.73%\n",
            "\ttrain_f1:0.774 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    8  6689]\n",
            " [   49 36401]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   0 1774]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.93it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:0.434 valid_loss:0.422\n",
            "\ttrain_acc:84.49% valid_acc:84.73%\n",
            "\ttrain_f1:0.774 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6683]\n",
            " [    0 36418]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   0 1774]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.81it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:0.415 valid_loss:0.406\n",
            "\ttrain_acc:84.57% valid_acc:84.73%\n",
            "\ttrain_f1:0.775 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6686]\n",
            " [    0 36638]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   0 1774]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.96it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.397 valid_loss:0.388\n",
            "\ttrain_acc:84.51% valid_acc:84.73%\n",
            "\ttrain_f1:0.774 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6699]\n",
            " [    0 36535]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   0 1774]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.370 valid_loss:0.353\n",
            "\ttrain_acc:84.48% valid_acc:84.73%\n",
            "\ttrain_f1:0.774 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6697]\n",
            " [    0 36454]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   0 1774]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.79it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.331 valid_loss:0.311\n",
            "\ttrain_acc:84.53% valid_acc:84.74%\n",
            "\ttrain_f1:0.774 valid_f1:0.777\n",
            "\ttrain_confusion_matrix:\n",
            "[[    0  6702]\n",
            " [    0 36615]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[   1 1773]\n",
            " [   0 9841]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.82it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.295 valid_loss:0.281\n",
            "\ttrain_acc:85.55% valid_acc:87.25%\n",
            "\ttrain_f1:0.810 valid_f1:0.853\n",
            "\ttrain_confusion_matrix:\n",
            "[[  800  5860]\n",
            " [  401 36280]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 565 1209]\n",
            " [ 272 9569]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.80it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.263 valid_loss:0.250\n",
            "\ttrain_acc:88.25% valid_acc:89.80%\n",
            "\ttrain_f1:0.870 valid_f1:0.891\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 2792  3883]\n",
            " [ 1178 35220]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 932  842]\n",
            " [ 343 9498]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.87it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.231 valid_loss:0.226\n",
            "\ttrain_acc:90.77% valid_acc:91.12%\n",
            "\ttrain_f1:0.904 valid_f1:0.908\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4081  2553]\n",
            " [ 1432 35123]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1102  672]\n",
            " [ 359 9482]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.85it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.208 valid_loss:0.212\n",
            "\ttrain_acc:91.68% valid_acc:91.39%\n",
            "\ttrain_f1:0.915 valid_f1:0.914\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4507  2148]\n",
            " [ 1447 35126]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1254  520]\n",
            " [ 480 9361]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.84it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.196 valid_loss:0.199\n",
            "\ttrain_acc:92.31% valid_acc:91.82%\n",
            "\ttrain_f1:0.922 valid_f1:0.918\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4779  1882]\n",
            " [ 1433 35006]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1272  502]\n",
            " [ 448 9393]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.81it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.188 valid_loss:0.196\n",
            "\ttrain_acc:92.54% valid_acc:91.92%\n",
            "\ttrain_f1:0.924 valid_f1:0.919\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4870  1815]\n",
            " [ 1405 35099]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1303  471]\n",
            " [ 467 9374]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.81it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.183 valid_loss:0.193\n",
            "\ttrain_acc:92.67% valid_acc:92.22%\n",
            "\ttrain_f1:0.926 valid_f1:0.921\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4934  1772]\n",
            " [ 1402 35198]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1255  519]\n",
            " [ 385 9456]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.96it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.177 valid_loss:0.191\n",
            "\ttrain_acc:93.06% valid_acc:92.45%\n",
            "\ttrain_f1:0.930 valid_f1:0.924\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4996  1640]\n",
            " [ 1340 34977]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1299  475]\n",
            " [ 402 9439]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.97it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.173 valid_loss:0.184\n",
            "\ttrain_acc:93.17% valid_acc:92.56%\n",
            "\ttrain_f1:0.931 valid_f1:0.926\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 4990  1684]\n",
            " [ 1274 35340]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1337  437]\n",
            " [ 427 9414]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  2.00it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.174 valid_loss:0.182\n",
            "\ttrain_acc:93.11% valid_acc:92.84%\n",
            "\ttrain_f1:0.930 valid_f1:0.928\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5053  1637]\n",
            " [ 1343 35200]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1327  447]\n",
            " [ 385 9456]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  2.03it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.169 valid_loss:0.184\n",
            "\ttrain_acc:93.51% valid_acc:92.64%\n",
            "\ttrain_f1:0.934 valid_f1:0.926\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5068  1601]\n",
            " [ 1203 35307]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1330  444]\n",
            " [ 411 9430]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.164 valid_loss:0.183\n",
            "\ttrain_acc:93.66% valid_acc:92.88%\n",
            "\ttrain_f1:0.936 valid_f1:0.928\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5056  1562]\n",
            " [ 1177 35384]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1321  453]\n",
            " [ 374 9467]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.94it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.163 valid_loss:0.179\n",
            "\ttrain_acc:93.66% valid_acc:93.19%\n",
            "\ttrain_f1:0.936 valid_f1:0.930\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5187  1510]\n",
            " [ 1224 35235]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1269  505]\n",
            " [ 286 9555]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.159 valid_loss:0.177\n",
            "\ttrain_acc:93.66% valid_acc:92.95%\n",
            "\ttrain_f1:0.936 valid_f1:0.929\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 5048  1586]\n",
            " [ 1152 35372]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[1327  447]\n",
            " [ 372 9469]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"aspact_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"aspact_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['aspact_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['aspact_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8, 0.95)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['aspact_train_acc'], label='train accuracy')\n",
        "plt.plot(history['aspact_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.8, 0.95])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AT       0.77      0.87      0.82      1774\n",
            "         NAT       0.98      0.95      0.96      9841\n",
            "\n",
            "    accuracy                           0.94     11615\n",
            "   macro avg       0.87      0.91      0.89     11615\n",
            "weighted avg       0.94      0.94      0.94     11615\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_aspect_tags = []\n",
        "    final_true_aspect_tags = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_aspect_tags.extend(pred_tags)\n",
        "            final_true_aspect_tags.extend(label)\n",
        "\n",
        "    final_pred_aspect_tags = torch.stack(final_pred_aspect_tags).cpu()\n",
        "    final_true_aspect_tags = torch.stack(final_true_aspect_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_aspect_tags, final_pred_aspect_tags, \n",
        "                                target_names=encoder.classes_))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
