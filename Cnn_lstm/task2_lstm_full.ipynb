{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model_task2_full.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "outputs": [],
      "source": [
        "from ast import FloorDiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## No clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n",
            "num of polarity tags: 4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# replace all -1 to 2 since pytorch cannot handle negative\n",
        "# so, 2 now means negative polarity\n",
        "df.polarity = df.polarity.replace(-1,2)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "polarity_tags = df.groupby(\"num\")[\"polarity\"].apply(list).values\n",
        "\n",
        "polarity_unique_values = df.polarity.unique()\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "print('num of polarity tags: {}'.format(len(polarity_unique_values)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))\n",
        "print(len(polarity_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, pol_tags, sent_len=85):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                if aspect_tags[sx][wx] == 0:\n",
        "                    mask[sx, wx] = 1\n",
        "                elif aspect_tags[sx][wx] == 1:\n",
        "                    mask[sx, wx] = 0\n",
        "                train_y[sx, wx] = pol_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label, num_tag):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, num_tag)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        seq_lengths = np.sum(np.array(x_train) !=0, axis=1)\n",
        "\n",
        "        x_emb_pack = torch.nn.utils.rnn.pack_padded_sequence(x_emb,seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output , (h_n, _) = self.lstm(x_emb_pack.float())\n",
        "        \n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=85)  \n",
        "\n",
        "        out = out[0] \n",
        "\n",
        "        #print(out.shape)\n",
        "        #output, (h_n, _) = self.lstm(x_emb.float())\n",
        "\n",
        "        out = self.dense(out)\n",
        "\n",
        "        out = torch.nn.functional.log_softmax(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, polarity_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_POLARITY_TAGS = 3\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding,  num_classes=3), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (gen_embedding): Embedding(6620, 300)\n",
            "  (lstm): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
            "  (dense): Linear(in_features=300, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:1.081 valid_loss:1.063\n",
            "\ttrain_acc:44.87% valid_acc:51.35%\n",
            "\ttrain_f1:0.473 valid_f1:0.508\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 194  679  414]\n",
            " [ 566 2083  900]\n",
            " [ 251  824  681]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 39 198 117]\n",
            " [ 49 641 217]\n",
            " [ 40 247 236]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.59it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:1.039 valid_loss:1.033\n",
            "\ttrain_acc:54.29% valid_acc:53.59%\n",
            "\ttrain_f1:0.532 valid_f1:0.522\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 103  732  437]\n",
            " [ 194 2630  739]\n",
            " [  76  839  851]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 34 205 115]\n",
            " [ 33 679 195]\n",
            " [ 36 244 243]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.62it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:1.005 valid_loss:1.006\n",
            "\ttrain_acc:56.72% valid_acc:55.94%\n",
            "\ttrain_f1:0.551 valid_f1:0.541\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 115  727  435]\n",
            " [ 153 2719  664]\n",
            " [  62  809  901]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 38 205 111]\n",
            " [ 26 701 180]\n",
            " [ 38 226 259]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.964 valid_loss:0.980\n",
            "\ttrain_acc:59.15% valid_acc:57.51%\n",
            "\ttrain_f1:0.571 valid_f1:0.553\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 117  734  420]\n",
            " [ 116 2871  581]\n",
            " [  55  786  910]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 39 207 108]\n",
            " [ 24 721 162]\n",
            " [ 34 223 266]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.70it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.925 valid_loss:0.953\n",
            "\ttrain_acc:61.22% valid_acc:58.80%\n",
            "\ttrain_f1:0.590 valid_f1:0.563\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 148  734  406]\n",
            " [  93 2915  489]\n",
            " [  67  744  935]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 38 202 114]\n",
            " [ 19 738 150]\n",
            " [ 40 210 273]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.76it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.863 valid_loss:0.929\n",
            "\ttrain_acc:64.20% valid_acc:59.59%\n",
            "\ttrain_f1:0.631 valid_f1:0.587\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 213  647  399]\n",
            " [ 107 3024  425]\n",
            " [  84  691  982]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 56 190 108]\n",
            " [ 22 746 139]\n",
            " [ 43 219 261]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.67it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.825 valid_loss:0.891\n",
            "\ttrain_acc:66.22% valid_acc:61.49%\n",
            "\ttrain_f1:0.662 valid_f1:0.616\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 291  573  405]\n",
            " [ 138 3076  353]\n",
            " [ 119  633  987]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 77 177 100]\n",
            " [ 25 753 129]\n",
            " [ 53 203 267]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.56it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.795 valid_loss:0.877\n",
            "\ttrain_acc:67.52% valid_acc:62.50%\n",
            "\ttrain_f1:0.675 valid_f1:0.630\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 331  570  385]\n",
            " [ 132 3112  313]\n",
            " [ 125  622 1020]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 88 166 100]\n",
            " [ 33 755 119]\n",
            " [ 62 189 272]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.49it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.762 valid_loss:0.841\n",
            "\ttrain_acc:68.88% valid_acc:64.18%\n",
            "\ttrain_f1:0.692 valid_f1:0.640\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 368  528  380]\n",
            " [ 145 3107  323]\n",
            " [ 167  513 1076]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 87 165 102]\n",
            " [ 37 770 100]\n",
            " [ 59 176 288]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.50it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.731 valid_loss:0.830\n",
            "\ttrain_acc:70.03% valid_acc:64.91%\n",
            "\ttrain_f1:0.704 valid_f1:0.650\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 400  495  378]\n",
            " [ 152 3086  313]\n",
            " [ 186  451 1129]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 93 164  97]\n",
            " [ 35 784  88]\n",
            " [ 61 181 281]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.34it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.707 valid_loss:0.817\n",
            "\ttrain_acc:70.98% valid_acc:65.13%\n",
            "\ttrain_f1:0.715 valid_f1:0.658\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 426  462  365]\n",
            " [ 182 3090  285]\n",
            " [ 207  412 1163]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[103 154  97]\n",
            " [ 39 775  93]\n",
            " [ 67 172 284]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.18it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.685 valid_loss:0.797\n",
            "\ttrain_acc:71.85% valid_acc:66.03%\n",
            "\ttrain_f1:0.722 valid_f1:0.665\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 453  477  350]\n",
            " [ 166 3125  246]\n",
            " [ 204  409 1149]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[106 149  99]\n",
            " [ 42 773  92]\n",
            " [ 68 156 299]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.56it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.663 valid_loss:0.784\n",
            "\ttrain_acc:73.17% valid_acc:65.75%\n",
            "\ttrain_f1:0.740 valid_f1:0.662\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 509  439  341]\n",
            " [ 163 3163  236]\n",
            " [ 222  373 1166]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[107 148  99]\n",
            " [ 53 763  91]\n",
            " [ 79 141 303]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.23it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.644 valid_loss:0.799\n",
            "\ttrain_acc:73.67% valid_acc:66.54%\n",
            "\ttrain_f1:0.745 valid_f1:0.673\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 540  428  324]\n",
            " [ 174 3125  240]\n",
            " [ 227  339 1182]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[115 152  87]\n",
            " [ 42 781  84]\n",
            " [ 74 158 291]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.32it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.632 valid_loss:0.783\n",
            "\ttrain_acc:74.39% valid_acc:66.70%\n",
            "\ttrain_f1:0.758 valid_f1:0.676\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 566  401  303]\n",
            " [ 159 3175  233]\n",
            " [ 255  343 1179]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[120 140  94]\n",
            " [ 50 769  88]\n",
            " [ 79 143 301]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.29it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.616 valid_loss:0.780\n",
            "\ttrain_acc:74.87% valid_acc:67.04%\n",
            "\ttrain_f1:0.759 valid_f1:0.680\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 565  403  302]\n",
            " [ 168 3150  248]\n",
            " [ 223  310 1214]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[122 139  93]\n",
            " [ 46 772  89]\n",
            " [ 76 145 302]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.18it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.594 valid_loss:0.773\n",
            "\ttrain_acc:76.14% valid_acc:67.54%\n",
            "\ttrain_f1:0.772 valid_f1:0.682\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 593  386  265]\n",
            " [ 171 3189  206]\n",
            " [ 239  303 1229]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[122 135  97]\n",
            " [ 50 767  90]\n",
            " [ 68 139 316]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:10<00:00,  2.04it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.590 valid_loss:0.786\n",
            "\ttrain_acc:76.23% valid_acc:67.15%\n",
            "\ttrain_f1:0.775 valid_f1:0.683\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 616  357  319]\n",
            " [ 161 3193  227]\n",
            " [ 223  286 1235]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[121 140  93]\n",
            " [ 40 785  82]\n",
            " [ 75 156 292]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.96it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.574 valid_loss:0.779\n",
            "\ttrain_acc:76.88% valid_acc:67.43%\n",
            "\ttrain_f1:0.783 valid_f1:0.691\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 632  362  294]\n",
            " [ 142 3241  213]\n",
            " [ 230  289 1216]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[142 133  79]\n",
            " [ 62 771  74]\n",
            " [ 90 143 290]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.86it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.555 valid_loss:0.783\n",
            "\ttrain_acc:77.70% valid_acc:67.77%\n",
            "\ttrain_f1:0.790 valid_f1:0.686\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 649  335  266]\n",
            " [ 159 3207  199]\n",
            " [ 231  283 1277]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[129 129  96]\n",
            " [ 53 759  95]\n",
            " [ 65 137 321]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"polarity_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"polarity_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label, 3)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label, 3)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['polarity_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['polarity_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3, 1.0)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['polarity_train_acc'], label='train accuracy')\n",
        "plt.plot(history['polarity_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model_task2_full.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2928943613.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neutral       0.51      0.34      0.41       354\n",
            "    Positive       0.74      0.85      0.79       907\n",
            "    Negative       0.63      0.60      0.62       523\n",
            "\n",
            "    accuracy                           0.68      1784\n",
            "   macro avg       0.62      0.60      0.60      1784\n",
            "weighted avg       0.66      0.68      0.66      1784\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_polarity_tags = []\n",
        "    final_true_polarity_tags = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_polarity_tags.extend(pred_tags)\n",
        "            final_true_polarity_tags.extend(label)\n",
        "\n",
        "    final_pred_polarity_tags = torch.stack(final_pred_polarity_tags).cpu()\n",
        "    final_true_polarity_tags = torch.stack(final_true_polarity_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_polarity_tags, final_pred_polarity_tags, \n",
        "                                target_names=[\"Neutral\", \"Positive\", \"Negative\"]))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_task2_full.bin'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model_task2_full_clean.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task2_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n",
            "num of polarity tags: 4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# replace all -1 to 2 since pytorch cannot handle negative\n",
        "# so, 2 now means negative polarity\n",
        "df.polarity = df.polarity.replace(-1,2)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "polarity_tags = df.groupby(\"num\")[\"polarity\"].apply(list).values\n",
        "\n",
        "polarity_unique_values = df.polarity.unique()\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "print('num of polarity tags: {}'.format(len(polarity_unique_values)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))\n",
        "print(len(polarity_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, pol_tags, sent_len=85):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                if aspect_tags[sx][wx] == 0:\n",
        "                    mask[sx, wx] = 1\n",
        "                elif aspect_tags[sx][wx] == 1:\n",
        "                    mask[sx, wx] = 0\n",
        "                train_y[sx, wx] = pol_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label, num_tag):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, num_tag)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nclass Model(torch.nn.Module):\\n    def __init__(self, gen_emb, num_classes=3):\\n        super(Model, self).__init__()\\n        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\\n        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\\n        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\\n        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\\n\\n    def forward(self, x_train):\\n        x_emb = self.gen_embedding(x_train)\\n\\n        output, (h_n, _) = self.lstm(x_emb.float())\\n        out = self.dense(output)\\n\\n        return out\\n'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        output, (h_n, _) = self.lstm(x_emb.float())\n",
        "        out = self.dense(output)\n",
        "\n",
        "        return out\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        seq_lengths = np.sum(np.array(x_train) !=0, axis=1)\n",
        "\n",
        "        x_emb_pack = torch.nn.utils.rnn.pack_padded_sequence(x_emb,seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output , (h_n, _) = self.lstm(x_emb_pack.float())\n",
        "        \n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=85)  \n",
        "\n",
        "        out = out[0] \n",
        "\n",
        "        out = self.dense(out)\n",
        "\n",
        "        out = torch.nn.functional.log_softmax(out)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, polarity_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_POLARITY_TAGS = 3\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding,  num_classes=3), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:11<00:00,  1.89it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:1.074 valid_loss:1.050\n",
            "\ttrain_acc:50.42% valid_acc:55.66%\n",
            "\ttrain_f1:0.531 valid_f1:0.541\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 164  813  224]\n",
            " [ 409 2636  394]\n",
            " [ 262 1092  448]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 37 239 118]\n",
            " [ 63 790  94]\n",
            " [ 35 254 181]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.32it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:1.037 valid_loss:1.015\n",
            "\ttrain_acc:56.57% valid_acc:56.10%\n",
            "\ttrain_f1:0.552 valid_f1:0.538\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 108  773  330]\n",
            " [ 188 2763  503]\n",
            " [ 100  922  797]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 26 210 158]\n",
            " [ 38 776 133]\n",
            " [ 22 234 214]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.36it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:1.000 valid_loss:0.982\n",
            "\ttrain_acc:58.50% valid_acc:56.82%\n",
            "\ttrain_f1:0.557 valid_f1:0.538\n",
            "\ttrain_confusion_matrix:\n",
            "[[  74  725  406]\n",
            " [ 109 2789  564]\n",
            " [  59  821  921]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 23 202 169]\n",
            " [ 27 773 147]\n",
            " [ 12 225 233]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.17it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.962 valid_loss:0.944\n",
            "\ttrain_acc:60.18% valid_acc:58.26%\n",
            "\ttrain_f1:0.568 valid_f1:0.553\n",
            "\ttrain_confusion_matrix:\n",
            "[[  70  699  422]\n",
            " [  83 2834  551]\n",
            " [  35  784  986]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 28 202 164]\n",
            " [ 24 789 134]\n",
            " [ 15 217 238]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:10<00:00,  2.09it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.926 valid_loss:0.902\n",
            "\ttrain_acc:60.96% valid_acc:60.96%\n",
            "\ttrain_f1:0.573 valid_f1:0.583\n",
            "\ttrain_confusion_matrix:\n",
            "[[  82  712  421]\n",
            " [  71 2848  504]\n",
            " [  39  773 1005]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 40 200 154]\n",
            " [ 18 819 110]\n",
            " [ 26 199 245]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.21it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.887 valid_loss:0.865\n",
            "\ttrain_acc:62.44% valid_acc:62.84%\n",
            "\ttrain_f1:0.598 valid_f1:0.610\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 128  673  417]\n",
            " [  67 2901  461]\n",
            " [  52  754 1000]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 56 194 144]\n",
            " [ 19 839  89]\n",
            " [ 29 198 243]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.30it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.849 valid_loss:0.831\n",
            "\ttrain_acc:64.09% valid_acc:64.66%\n",
            "\ttrain_f1:0.617 valid_f1:0.632\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 155  670  389]\n",
            " [  90 2981  359]\n",
            " [  72  731  989]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 68 185 141]\n",
            " [ 21 850  76]\n",
            " [ 34 183 253]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.33it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.813 valid_loss:0.807\n",
            "\ttrain_acc:66.15% valid_acc:64.61%\n",
            "\ttrain_f1:0.639 valid_f1:0.634\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 179  608  409]\n",
            " [ 111 3018  336]\n",
            " [  74  643 1066]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 75 183 136]\n",
            " [ 20 829  98]\n",
            " [ 34 170 266]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.15it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.790 valid_loss:0.780\n",
            "\ttrain_acc:67.15% valid_acc:65.71%\n",
            "\ttrain_f1:0.645 valid_f1:0.647\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 203  606  424]\n",
            " [ 109 3000  322]\n",
            " [  88  578 1145]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 81 168 145]\n",
            " [ 23 830  94]\n",
            " [ 36 155 279]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.14it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.764 valid_loss:0.761\n",
            "\ttrain_acc:68.10% valid_acc:66.54%\n",
            "\ttrain_f1:0.659 valid_f1:0.652\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 215  548  442]\n",
            " [  97 2968  356]\n",
            " [ 103  511 1208]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 81 174 139]\n",
            " [ 23 845  79]\n",
            " [ 42 149 279]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:10<00:00,  2.06it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.737 valid_loss:0.744\n",
            "\ttrain_acc:69.63% valid_acc:67.26%\n",
            "\ttrain_f1:0.675 valid_f1:0.668\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 250  572  390]\n",
            " [  96 3049  302]\n",
            " [ 110  489 1192]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 99 157 138]\n",
            " [ 29 822  96]\n",
            " [ 48 125 297]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.17it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.724 valid_loss:0.730\n",
            "\ttrain_acc:69.95% valid_acc:68.75%\n",
            "\ttrain_f1:0.686 valid_f1:0.691\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 287  524  408]\n",
            " [  90 3023  330]\n",
            " [ 118  469 1204]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[116 138 140]\n",
            " [ 33 824  90]\n",
            " [ 57 108 305]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:10<00:00,  1.95it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.702 valid_loss:0.719\n",
            "\ttrain_acc:70.68% valid_acc:68.53%\n",
            "\ttrain_f1:0.701 valid_f1:0.681\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 333  486  398]\n",
            " [ 128 3034  269]\n",
            " [ 165  441 1182]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[104 154 136]\n",
            " [ 30 838  79]\n",
            " [ 59 112 299]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.27it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.680 valid_loss:0.713\n",
            "\ttrain_acc:72.50% valid_acc:68.75%\n",
            "\ttrain_f1:0.718 valid_f1:0.686\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 374  462  379]\n",
            " [ 130 3045  266]\n",
            " [ 134  401 1252]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[112 154 128]\n",
            " [ 33 833  81]\n",
            " [ 65 105 300]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:09<00:00,  2.17it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.671 valid_loss:0.704\n",
            "\ttrain_acc:73.04% valid_acc:68.64%\n",
            "\ttrain_f1:0.729 valid_f1:0.688\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 407  439  355]\n",
            " [ 134 3061  266]\n",
            " [ 153  393 1245]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[116 149 129]\n",
            " [ 39 832  76]\n",
            " [ 73 102 295]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.73it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.644 valid_loss:0.692\n",
            "\ttrain_acc:74.23% valid_acc:69.85%\n",
            "\ttrain_f1:0.742 valid_f1:0.709\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 433  420  338]\n",
            " [ 140 3086  248]\n",
            " [ 161  352 1259]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[142 134 118]\n",
            " [ 43 825  79]\n",
            " [ 75  97 298]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.58it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.634 valid_loss:0.688\n",
            "\ttrain_acc:74.85% valid_acc:70.35%\n",
            "\ttrain_f1:0.750 valid_f1:0.715\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 475  391  342]\n",
            " [ 141 3067  240]\n",
            " [ 163  353 1309]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[149 137 108]\n",
            " [ 40 829  78]\n",
            " [ 77  97 296]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.52it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.622 valid_loss:0.686\n",
            "\ttrain_acc:75.32% valid_acc:70.96%\n",
            "\ttrain_f1:0.760 valid_f1:0.727\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 529  366  320]\n",
            " [ 148 3055  233]\n",
            " [ 182  340 1265]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[164 110 120]\n",
            " [ 50 810  87]\n",
            " [ 74  85 311]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.38it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.606 valid_loss:0.684\n",
            "\ttrain_acc:76.41% valid_acc:70.62%\n",
            "\ttrain_f1:0.768 valid_f1:0.733\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 534  364  324]\n",
            " [ 138 3095  227]\n",
            " [ 151  323 1317]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[182 100 112]\n",
            " [ 54 801  92]\n",
            " [ 83  91 296]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.41it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.590 valid_loss:0.681\n",
            "\ttrain_acc:76.69% valid_acc:71.29%\n",
            "\ttrain_f1:0.775 valid_f1:0.727\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 562  336  296]\n",
            " [ 163 3089  215]\n",
            " [ 183  315 1309]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[159 124 111]\n",
            " [ 38 826  83]\n",
            " [ 70  94 306]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"polarity_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"polarity_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label, 3)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label, 3)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['polarity_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['polarity_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3, 1.0)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['polarity_train_acc'], label='train accuracy')\n",
        "plt.plot(history['polarity_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model_task2_full_clean.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_14909/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neutral       0.60      0.40      0.48       394\n",
            "    Positive       0.79      0.87      0.83       947\n",
            "    Negative       0.61      0.65      0.63       470\n",
            "\n",
            "    accuracy                           0.71      1811\n",
            "   macro avg       0.67      0.64      0.65      1811\n",
            "weighted avg       0.70      0.71      0.70      1811\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_polarity_tags = []\n",
        "    final_true_polarity_tags = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_polarity_tags.extend(pred_tags)\n",
        "            final_true_polarity_tags.extend(label)\n",
        "\n",
        "    final_pred_polarity_tags = torch.stack(final_pred_polarity_tags).cpu()\n",
        "    final_true_polarity_tags = torch.stack(final_true_polarity_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_polarity_tags, final_pred_polarity_tags, \n",
        "                                target_names=[\"Neutral\", \"Positive\", \"Negative\"]))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_task2_full_clean.bin'"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_PATH"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
