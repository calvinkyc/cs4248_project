{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model_task2_full.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVlBRwsjo3A9"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAicdvsynjtH"
      },
      "outputs": [],
      "source": [
        "from ast import FloorDiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## No clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n",
            "num of polarity tags: 4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# replace all -1 to 2 since pytorch cannot handle negative\n",
        "# so, 2 now means negative polarity\n",
        "df.polarity = df.polarity.replace(-1,2)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "polarity_tags = df.groupby(\"num\")[\"polarity\"].apply(list).values\n",
        "\n",
        "polarity_unique_values = df.polarity.unique()\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "print('num of polarity tags: {}'.format(len(polarity_unique_values)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))\n",
        "print(len(polarity_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, pol_tags, sent_len=85):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                if aspect_tags[sx][wx] == 0:\n",
        "                    mask[sx, wx] = 1\n",
        "                elif aspect_tags[sx][wx] == 1:\n",
        "                    mask[sx, wx] = 0\n",
        "                train_y[sx, wx] = pol_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label, num_tag):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, num_tag)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        seq_lengths = np.sum(np.array(x_train) !=0, axis=1)\n",
        "\n",
        "        x_emb_pack = torch.nn.utils.rnn.pack_padded_sequence(x_emb,seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output , (h_n, _) = self.lstm(x_emb_pack.float())\n",
        "        \n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=85)  \n",
        "\n",
        "        out = out[0] \n",
        "\n",
        "        #print(out.shape)\n",
        "        #output, (h_n, _) = self.lstm(x_emb.float())\n",
        "\n",
        "        out = self.dense(out)\n",
        "\n",
        "        out = torch.nn.functional.log_softmax(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, polarity_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_POLARITY_TAGS = 3\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding,  num_classes=3), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (gen_embedding): Embedding(6620, 300)\n",
            "  (lstm): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
            "  (dense): Linear(in_features=300, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.53it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:1.079 valid_loss:1.059\n",
            "\ttrain_acc:47.65% valid_acc:53.81%\n",
            "\ttrain_f1:0.520 valid_f1:0.570\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 266  755  261]\n",
            " [ 542 2388  560]\n",
            " [ 436  898  488]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 68 204  60]\n",
            " [ 99 736 130]\n",
            " [ 76 243 142]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.51it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:1.042 valid_loss:1.026\n",
            "\ttrain_acc:55.30% valid_acc:55.01%\n",
            "\ttrain_f1:0.550 valid_f1:0.552\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 161  771  362]\n",
            " [ 190 2694  580]\n",
            " [ 162  889  800]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 32 183 117]\n",
            " [ 40 722 203]\n",
            " [ 30 218 213]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.34it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:1.006 valid_loss:0.994\n",
            "\ttrain_acc:58.32% valid_acc:56.66%\n",
            "\ttrain_f1:0.567 valid_f1:0.559\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 148  754  374]\n",
            " [ 141 2769  588]\n",
            " [  81  805  921]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 29 186 117]\n",
            " [ 40 739 186]\n",
            " [ 25 208 228]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.975 valid_loss:0.959\n",
            "\ttrain_acc:59.27% valid_acc:58.13%\n",
            "\ttrain_f1:0.575 valid_f1:0.571\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 156  726  408]\n",
            " [ 113 2809  595]\n",
            " [  71  795  975]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 32 184 116]\n",
            " [ 37 747 181]\n",
            " [ 27 191 243]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.43it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.942 valid_loss:0.920\n",
            "\ttrain_acc:60.85% valid_acc:60.98%\n",
            "\ttrain_f1:0.589 valid_f1:0.596\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 169  717  417]\n",
            " [  99 2878  516]\n",
            " [  66  779  984]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 37 182 113]\n",
            " [ 34 789 142]\n",
            " [ 24 191 246]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.901 valid_loss:0.876\n",
            "\ttrain_acc:62.39% valid_acc:63.03%\n",
            "\ttrain_f1:0.605 valid_f1:0.627\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 195  689  401]\n",
            " [ 113 2895  468]\n",
            " [  76  735 1028]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 52 161 119]\n",
            " [ 33 808 124]\n",
            " [ 29 184 248]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.854 valid_loss:0.843\n",
            "\ttrain_acc:64.58% valid_acc:65.19%\n",
            "\ttrain_f1:0.634 valid_f1:0.657\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 241  667  387]\n",
            " [ 118 3026  386]\n",
            " [ 105  680 1004]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 71 154 107]\n",
            " [ 37 835  93]\n",
            " [ 36 185 240]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.67it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.816 valid_loss:0.818\n",
            "\ttrain_acc:65.86% valid_acc:66.44%\n",
            "\ttrain_f1:0.657 valid_f1:0.671\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 317  583  406]\n",
            " [ 142 3009  349]\n",
            " [ 122  660 1038]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 77 144 111]\n",
            " [ 41 838  86]\n",
            " [ 47 161 253]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.777 valid_loss:0.794\n",
            "\ttrain_acc:67.87% valid_acc:66.38%\n",
            "\ttrain_f1:0.675 valid_f1:0.674\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 334  570  367]\n",
            " [ 138 3054  326]\n",
            " [ 145  575 1092]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 83 136 113]\n",
            " [ 46 826  93]\n",
            " [ 51 152 258]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.62it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.752 valid_loss:0.777\n",
            "\ttrain_acc:69.09% valid_acc:66.33%\n",
            "\ttrain_f1:0.686 valid_f1:0.671\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 354  542  376]\n",
            " [ 153 3059  301]\n",
            " [ 161  512 1158]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 78 140 114]\n",
            " [ 44 828  93]\n",
            " [ 57 144 260]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.42it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.719 valid_loss:0.768\n",
            "\ttrain_acc:70.46% valid_acc:66.44%\n",
            "\ttrain_f1:0.697 valid_f1:0.676\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 394  530  382]\n",
            " [ 149 3051  284]\n",
            " [ 157  453 1219]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 80 135 117]\n",
            " [ 42 835  88]\n",
            " [ 60 148 253]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.698 valid_loss:0.758\n",
            "\ttrain_acc:71.38% valid_acc:67.35%\n",
            "\ttrain_f1:0.712 valid_f1:0.690\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 432  490  366]\n",
            " [ 163 3059  278]\n",
            " [ 179  412 1218]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 96 118 118]\n",
            " [ 57 827  81]\n",
            " [ 60 140 261]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.67it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.677 valid_loss:0.749\n",
            "\ttrain_acc:72.54% valid_acc:67.46%\n",
            "\ttrain_f1:0.727 valid_f1:0.692\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 483  470  348]\n",
            " [ 158 3098  251]\n",
            " [ 192  395 1210]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 97 121 114]\n",
            " [ 56 832  77]\n",
            " [ 66 138 257]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.62it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.654 valid_loss:0.749\n",
            "\ttrain_acc:73.37% valid_acc:68.03%\n",
            "\ttrain_f1:0.738 valid_f1:0.701\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 530  427  336]\n",
            " [ 178 3049  251]\n",
            " [ 186  379 1261]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[107 116 109]\n",
            " [ 57 832  76]\n",
            " [ 68 136 257]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.644 valid_loss:0.730\n",
            "\ttrain_acc:73.63% valid_acc:68.37%\n",
            "\ttrain_f1:0.748 valid_f1:0.702\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 589  386  313]\n",
            " [ 202 3035  248]\n",
            " [ 233  351 1216]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[106 115 111]\n",
            " [ 62 826  77]\n",
            " [ 70 121 270]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.52it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.621 valid_loss:0.735\n",
            "\ttrain_acc:74.36% valid_acc:69.11%\n",
            "\ttrain_f1:0.748 valid_f1:0.712\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 550  425  321]\n",
            " [ 166 3105  233]\n",
            " [ 215  341 1277]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[113 107 112]\n",
            " [ 53 825  87]\n",
            " [ 70 114 277]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.51it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.616 valid_loss:0.724\n",
            "\ttrain_acc:74.98% valid_acc:69.00%\n",
            "\ttrain_f1:0.759 valid_f1:0.710\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 610  369  290]\n",
            " [ 194 3056  262]\n",
            " [ 197  343 1293]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[111 112 109]\n",
            " [ 55 833  77]\n",
            " [ 72 120 269]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.44it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.596 valid_loss:0.728\n",
            "\ttrain_acc:75.63% valid_acc:69.74%\n",
            "\ttrain_f1:0.767 valid_f1:0.720\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 651  361  297]\n",
            " [ 173 3056  241]\n",
            " [ 210  329 1293]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[122 110 100]\n",
            " [ 53 837  75]\n",
            " [ 73 121 267]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:08<00:00,  2.60it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.580 valid_loss:0.722\n",
            "\ttrain_acc:76.39% valid_acc:69.34%\n",
            "\ttrain_f1:0.775 valid_f1:0.714\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 663  353  290]\n",
            " [ 181 3103  220]\n",
            " [ 222  298 1295]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[117 104 111]\n",
            " [ 56 817  92]\n",
            " [ 70 106 285]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.570 valid_loss:0.715\n",
            "\ttrain_acc:76.85% valid_acc:69.74%\n",
            "\ttrain_f1:0.777 valid_f1:0.722\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 648  336  315]\n",
            " [ 171 3065  227]\n",
            " [ 186  285 1333]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[128  97 107]\n",
            " [ 63 813  89]\n",
            " [ 73 103 285]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"polarity_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"polarity_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label, 3)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label, 3)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['polarity_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['polarity_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3, 1.0)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['polarity_train_acc'], label='train accuracy')\n",
        "plt.plot(history['polarity_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model_task2_full.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/1978988207.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neutral       0.48      0.39      0.43       332\n",
            "    Positive       0.80      0.84      0.82       965\n",
            "    Negative       0.59      0.62      0.61       461\n",
            "\n",
            "    accuracy                           0.70      1758\n",
            "   macro avg       0.63      0.62      0.62      1758\n",
            "weighted avg       0.69      0.70      0.69      1758\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_polarity_tags = []\n",
        "    final_true_polarity_tags = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_polarity_tags.extend(pred_tags)\n",
        "            final_true_polarity_tags.extend(label)\n",
        "\n",
        "    final_pred_polarity_tags = torch.stack(final_pred_polarity_tags).cpu()\n",
        "    final_true_polarity_tags = torch.stack(final_true_polarity_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_polarity_tags, final_pred_polarity_tags, \n",
        "                                target_names=[\"Neutral\", \"Positive\", \"Negative\"]))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_task2_full.bin'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Pp-VjL_5Xw",
        "outputId": "a043786f-12ea-4b95-f6bb-4f7c9288537a"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"./data/\"\n",
        "VALID_SIZE = .2\n",
        "MODEL_PATH = \"model_task2_full_clean.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dataset/data/restaurants_laptop_train_with_pos_task2_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "      <th>aspect_tag</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_1</td>\n",
              "      <td>I</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_1</td>\n",
              "      <td>charge</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_1</td>\n",
              "      <td>it</td>\n",
              "      <td>PRON</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_1</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_1</td>\n",
              "      <td>night</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NAT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num    text   pos aspect_tag  polarity\n",
              "0  s_1       I  PRON        NAT         0\n",
              "1  s_1  charge  VERB        NAT         0\n",
              "2  s_1      it  PRON        NAT         0\n",
              "3  s_1      at   ADP        NAT         0\n",
              "4  s_1   night  NOUN        NAT         0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of aspect tags: 2\n",
            "num of polarity tags: 4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# replace all -1 to 2 since pytorch cannot handle negative\n",
        "# so, 2 now means negative polarity\n",
        "df.polarity = df.polarity.replace(-1,2)\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
        "\n",
        "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
        "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
        "polarity_tags = df.groupby(\"num\")[\"polarity\"].apply(list).values\n",
        "\n",
        "polarity_unique_values = df.polarity.unique()\n",
        "\n",
        "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
        "print('num of polarity tags: {}'.format(len(polarity_unique_values)))\n",
        "\n",
        "np.where(encoder.classes_ == \"AT\")[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3501\n",
            "3501\n",
            "3501\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(len(aspect_tags))\n",
        "print(len(polarity_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n"
          ]
        }
      ],
      "source": [
        "print(max(map(lambda x: len(x), sentences)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dkojY_dde3K-"
      },
      "outputs": [],
      "source": [
        "# generate word_index list\n",
        "def build_vocab(df):\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(sorted(set(df.text.values))):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "def create_train_data_restaurant(sentences, word_idx, pol_tags, sent_len=85):\n",
        "    train_X = np.zeros((len(sentences), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(sentences), sent_len), np.int16)\n",
        "\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(sentences):\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(sent):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                if aspect_tags[sx][wx] == 0:\n",
        "                    mask[sx, wx] = 1\n",
        "                elif aspect_tags[sx][wx] == 1:\n",
        "                    mask[sx, wx] = 0\n",
        "                train_y[sx, wx] = pol_tags[sx][wx]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return (train_X, mask), train_y\n",
        "\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label, num_tag):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, num_tag)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nclass Model(torch.nn.Module):\\n    def __init__(self, gen_emb, num_classes=3):\\n        super(Model, self).__init__()\\n        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\\n        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\\n        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\\n        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\\n\\n    def forward(self, x_train):\\n        x_emb = self.gen_embedding(x_train)\\n\\n        output, (h_n, _) = self.lstm(x_emb.float())\\n        out = self.dense(output)\\n\\n        return out\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        output, (h_n, _) = self.lstm(x_emb.float())\n",
        "        out = self.dense(output)\n",
        "\n",
        "        return out\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbH1kXllslz",
        "outputId": "e54a1149-6949-4d20-e7eb-05dc01f35f7b"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, num_classes=3):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.lstm = nn.LSTM(gen_emb.shape[1], hidden_size=150, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dense = torch.nn.Linear(gen_emb.shape[1], num_classes)\n",
        "\n",
        "    def forward(self, x_train):\n",
        "        x_emb = self.gen_embedding(x_train)\n",
        "\n",
        "        seq_lengths = np.sum(np.array(x_train) !=0, axis=1)\n",
        "\n",
        "        x_emb_pack = torch.nn.utils.rnn.pack_padded_sequence(x_emb,seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output , (h_n, _) = self.lstm(x_emb_pack.float())\n",
        "        \n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=85)  \n",
        "\n",
        "        out = out[0] \n",
        "\n",
        "        out = self.dense(out)\n",
        "\n",
        "        out = torch.nn.functional.log_softmax(out)\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_indx = build_vocab(df)\n",
        "\n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)\n",
        "\n",
        "\n",
        "(X, mask), y = create_train_data_restaurant(sentences, word_indx, polarity_tags, sent_len=85)\n",
        "\n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid = train_test_split(X, mask, y, test_size=VALID_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:2800\n",
            "valid samples:701\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_POLARITY_TAGS = 3\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding,  num_classes=3), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.96it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:1.080 valid_loss:1.063\n",
            "\ttrain_acc:47.08% valid_acc:52.45%\n",
            "\ttrain_f1:0.480 valid_f1:0.518\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 162  618  504]\n",
            " [ 555 2060  875]\n",
            " [ 238  665  852]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 30 162 132]\n",
            " [ 46 616 229]\n",
            " [ 27 211 244]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.08it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:1.042 valid_loss:1.031\n",
            "\ttrain_acc:55.13% valid_acc:55.04%\n",
            "\ttrain_f1:0.529 valid_f1:0.542\n",
            "\ttrain_confusion_matrix:\n",
            "[[  84  705  494]\n",
            " [ 126 2575  791]\n",
            " [  72  760  963]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 33 161 130]\n",
            " [ 28 649 214]\n",
            " [ 20 210 252]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.02it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:1.006 valid_loss:1.000\n",
            "\ttrain_acc:56.90% valid_acc:56.51%\n",
            "\ttrain_f1:0.540 valid_f1:0.551\n",
            "\ttrain_confusion_matrix:\n",
            "[[  86  707  496]\n",
            " [ 104 2630  731]\n",
            " [  51  726 1000]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 31 162 131]\n",
            " [ 24 671 196]\n",
            " [ 14 211 257]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.96it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:0.967 valid_loss:0.964\n",
            "\ttrain_acc:59.33% valid_acc:59.63%\n",
            "\ttrain_f1:0.565 valid_f1:0.578\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 102  712  453]\n",
            " [  81 2786  634]\n",
            " [  53  738 1008]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 34 167 123]\n",
            " [ 18 717 156]\n",
            " [ 20 201 261]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.92it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:0.927 valid_loss:0.924\n",
            "\ttrain_acc:60.84% valid_acc:61.23%\n",
            "\ttrain_f1:0.585 valid_f1:0.596\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 130  706  436]\n",
            " [  94 2908  535]\n",
            " [  61  753  978]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 44 168 112]\n",
            " [ 33 732 126]\n",
            " [ 22 197 263]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:0.884 valid_loss:0.889\n",
            "\ttrain_acc:63.15% valid_acc:62.76%\n",
            "\ttrain_f1:0.615 valid_f1:0.612\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 195  694  376]\n",
            " [  89 2962  424]\n",
            " [  84  744  974]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 48 168 108]\n",
            " [ 32 759 100]\n",
            " [ 30 194 258]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.16it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:0.842 valid_loss:0.859\n",
            "\ttrain_acc:65.81% valid_acc:63.05%\n",
            "\ttrain_f1:0.652 valid_f1:0.623\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 276  599  391]\n",
            " [ 106 3008  371]\n",
            " [  90  680 1021]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 58 159 107]\n",
            " [ 43 755  93]\n",
            " [ 40 185 257]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.99it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:0.808 valid_loss:0.836\n",
            "\ttrain_acc:66.76% valid_acc:63.88%\n",
            "\ttrain_f1:0.662 valid_f1:0.638\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 302  610  369]\n",
            " [ 111 3060  341]\n",
            " [  93  665 1035]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 68 153 103]\n",
            " [ 42 761  88]\n",
            " [ 47 180 255]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.07it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:0.768 valid_loss:0.819\n",
            "\ttrain_acc:69.22% valid_acc:64.00%\n",
            "\ttrain_f1:0.680 valid_f1:0.639\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 315  589  351]\n",
            " [ 128 3093  296]\n",
            " [ 104  549 1129]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 72 149 103]\n",
            " [ 50 747  94]\n",
            " [ 55 160 267]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.28it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:0.734 valid_loss:0.808\n",
            "\ttrain_acc:70.40% valid_acc:65.00%\n",
            "\ttrain_f1:0.695 valid_f1:0.657\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 345  541  381]\n",
            " [ 120 3097  290]\n",
            " [ 113  497 1176]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[ 86 130 108]\n",
            " [ 53 738 100]\n",
            " [ 65 138 279]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:07<00:00,  2.90it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:0.707 valid_loss:0.803\n",
            "\ttrain_acc:71.49% valid_acc:64.76%\n",
            "\ttrain_f1:0.712 valid_f1:0.661\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 422  522  342]\n",
            " [ 154 3091  279]\n",
            " [ 169  403 1173]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[101 126  97]\n",
            " [ 71 726  94]\n",
            " [ 75 135 272]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.22it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:0.687 valid_loss:0.797\n",
            "\ttrain_acc:72.21% valid_acc:65.00%\n",
            "\ttrain_f1:0.718 valid_f1:0.664\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 437  491  355]\n",
            " [ 160 3069  246]\n",
            " [ 170  397 1220]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[104 124  96]\n",
            " [ 69 722 100]\n",
            " [ 73 132 277]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:06<00:00,  3.18it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:0.663 valid_loss:0.792\n",
            "\ttrain_acc:73.08% valid_acc:65.65%\n",
            "\ttrain_f1:0.734 valid_f1:0.676\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 472  447  319]\n",
            " [ 168 3095  260]\n",
            " [ 188  385 1229]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[111 124  89]\n",
            " [ 65 738  88]\n",
            " [ 86 131 265]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:04<00:00,  4.64it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:0.649 valid_loss:0.788\n",
            "\ttrain_acc:73.57% valid_acc:66.29%\n",
            "\ttrain_f1:0.736 valid_f1:0.684\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 502  451  324]\n",
            " [ 171 3069  251]\n",
            " [ 168  368 1253]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[118 118  88]\n",
            " [ 64 738  89]\n",
            " [ 88 125 269]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:03<00:00,  5.30it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:0.633 valid_loss:0.791\n",
            "\ttrain_acc:73.87% valid_acc:65.53%\n",
            "\ttrain_f1:0.746 valid_f1:0.675\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 554  412  309]\n",
            " [ 184 3075  237]\n",
            " [ 209  366 1226]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[116 109  99]\n",
            " [ 76 713 102]\n",
            " [ 85 114 283]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:03<00:00,  5.31it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:0.622 valid_loss:0.788\n",
            "\ttrain_acc:75.24% valid_acc:66.41%\n",
            "\ttrain_f1:0.761 valid_f1:0.688\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 583  363  331]\n",
            " [ 181 3093  241]\n",
            " [ 186  328 1277]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[126 117  81]\n",
            " [ 67 734  90]\n",
            " [ 93 122 267]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:04<00:00,  5.11it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:0.603 valid_loss:0.786\n",
            "\ttrain_acc:76.19% valid_acc:66.71%\n",
            "\ttrain_f1:0.771 valid_f1:0.692\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 589  345  306]\n",
            " [ 170 3094  240]\n",
            " [ 182  312 1292]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[129 114  81]\n",
            " [ 67 734  90]\n",
            " [ 96 117 269]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:03<00:00,  5.27it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:0.589 valid_loss:0.796\n",
            "\ttrain_acc:76.63% valid_acc:66.82%\n",
            "\ttrain_f1:0.776 valid_f1:0.695\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 632  353  293]\n",
            " [ 157 3107  221]\n",
            " [ 194  314 1284]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[129 110  85]\n",
            " [ 65 736  90]\n",
            " [ 99 114 269]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:04<00:00,  5.21it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:0.584 valid_loss:0.788\n",
            "\ttrain_acc:76.19% valid_acc:66.23%\n",
            "\ttrain_f1:0.773 valid_f1:0.692\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 634  341  321]\n",
            " [ 179 3099  236]\n",
            " [ 193  301 1293]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[133 112  79]\n",
            " [ 70 733  88]\n",
            " [110 114 258]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 21/21 [00:04<00:00,  5.20it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:0.565 valid_loss:0.787\n",
            "\ttrain_acc:77.68% valid_acc:66.65%\n",
            "\ttrain_f1:0.788 valid_f1:0.694\n",
            "\ttrain_confusion_matrix:\n",
            "[[ 676  314  289]\n",
            " [ 175 3094  201]\n",
            " [ 203  276 1304]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[136 106  82]\n",
            " [ 75 722  94]\n",
            " [102 107 273]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"train_loss\": list(),\n",
        "    \"polarity_train_acc\": list(),\n",
        "    \"valid_loss\": list(),\n",
        "    \"polarity_valid_acc\": list(),\n",
        "}\n",
        "\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits = model(feature)\n",
        "        loss = loss_fn(pred_logits, mask, label, 3)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label, 3)\n",
        "\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n",
        "\n",
        "    if avg_test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_loss = avg_test_loss    \n",
        "        \n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['polarity_train_acc'].append(avg_train_acc.cpu().numpy())\n",
        "    history['valid_loss'].append(avg_test_loss)\n",
        "    history['polarity_valid_acc'].append(avg_test_acc.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3, 1.0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history['polarity_train_acc'], label='train accuracy')\n",
        "plt.plot(history['polarity_valid_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.3, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading saved model from: model_task2_full_clean.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/3c/71b8pg6d7j5dht_5h_2n4n_c0000gn/T/ipykernel_12578/2772296862.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = torch.nn.functional.log_softmax(out)\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neutral       0.44      0.40      0.42       324\n",
            "    Positive       0.76      0.82      0.79       891\n",
            "    Negative       0.61      0.56      0.58       482\n",
            "\n",
            "    accuracy                           0.67      1697\n",
            "   macro avg       0.60      0.59      0.60      1697\n",
            "weighted avg       0.66      0.67      0.66      1697\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_classification_report(test_loader, model, model_path=None):\n",
        "    if model_path is not None: # load the saved model\n",
        "        print('Loading saved model from: {}'.format(model_path))\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    model = to_device(model, device)   \n",
        "    \n",
        "    model.eval()\n",
        "    final_pred_polarity_tags = []\n",
        "    final_true_polarity_tags = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            pred_tags = pred_tags[mask]\n",
        "            label = label[mask]\n",
        "\n",
        "            final_pred_polarity_tags.extend(pred_tags)\n",
        "            final_true_polarity_tags.extend(label)\n",
        "\n",
        "    final_pred_polarity_tags = torch.stack(final_pred_polarity_tags).cpu()\n",
        "    final_true_polarity_tags = torch.stack(final_true_polarity_tags).cpu()\n",
        "        \n",
        "    print(classification_report(final_true_polarity_tags, final_pred_polarity_tags, \n",
        "                                target_names=[\"Neutral\", \"Positive\", \"Negative\"]))\n",
        "    \n",
        "get_classification_report(test_loader, model, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_task2_full_clean.bin'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_PATH"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
