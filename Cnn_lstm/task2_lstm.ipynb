{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVlBRwsjo3A9",
        "outputId": "c73353d2-d0e0-46ab-cb7b-742f72aa0104"
      },
      "outputs": [],
      "source": [
        "#!pip install fasttext\n",
        "#!pip install transformers\n",
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "#!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0eHpAwJZTqff"
      },
      "outputs": [],
      "source": [
        "DATA_DIR =  './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f_ihgOgKQQ5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.optim import AdamW\n",
        "from fasttext import load_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MIElcOeLQVR0"
      },
      "outputs": [],
      "source": [
        "VALID_SIZE = .2\n",
        "\n",
        "\n",
        "# generate word_index list\n",
        "def build_vocab(data_dir, plain=[]):\n",
        "    \"\"\"plain is a empty str file which will record all text from official dataset\"\"\"\n",
        "    for fn in os.listdir(data_dir):\n",
        "        if fn.endswith('.xml'):\n",
        "            with open(data_dir + fn) as f:\n",
        "                dom = ET.parse(f)\n",
        "                root = dom.getroot()\n",
        "                for sent in root.iter(\"sentence\"):\n",
        "                    text = sent.find('text').text.lower()\n",
        "                    token = word_tokenize(text)\n",
        "                    plain = plain + token\n",
        "    vocab = sorted(set(plain))\n",
        "    with open(os.path.join(data_dir, \"plain.txt\"), \"w+\", encoding=\"utf8\") as f:\n",
        "        for v in vocab:\n",
        "            f.write(f\"{v}\\n\")\n",
        "    word_idx = {}\n",
        "    for idx, word in enumerate(vocab):\n",
        "        word_idx[word] = idx + 1\n",
        "    return word_idx\n",
        "\n",
        "\n",
        "def gen_np_embedding(fn, word_idx, dim=100, emb=False):\n",
        "    if emb:\n",
        "        model = load_model(fn + \".bin\")\n",
        "    embedding = np.zeros((len(word_idx) + 2, dim))\n",
        "\n",
        "    with open(fn, encoding=\"utf8\") as f:\n",
        "        for l in f:\n",
        "            # for each line, get the word and its vector\n",
        "            rec = l.rstrip().split(' ')\n",
        "            if len(rec) == 2:  # skip the first line.\n",
        "                continue\n",
        "                # if the word in word_idx, fill the embedding\n",
        "            if rec[0] in word_idx:\n",
        "                embedding[word_idx[rec[0]]] = np.array([float(r) for r in rec[1:]])\n",
        "    for w in word_idx:\n",
        "        if embedding[word_idx[w]].sum() == 0.:\n",
        "            if emb:\n",
        "                embedding[word_idx[w]] = model.get_word_vector(w)\n",
        "    return embedding\n",
        "\n",
        "dict_polarity = {'non-aspect':0, 'positive':1,'neutral':2, 'negative':3,  'conflict':4}\n",
        "def create_train_data_restaurant(fn, word_idx, sent_len=83):\n",
        "    dom = ET.parse(fn)\n",
        "    root = dom.getroot()\n",
        "    train_X = np.zeros((len(root), sent_len), np.int16)\n",
        "    mask = np.zeros_like(train_X)\n",
        "\n",
        "    train_y = np.zeros((len(root), sent_len), np.int16)\n",
        "    train_y_polarity = np.zeros((len(root), sent_len), np.int16)\n",
        "    take = np.ones(len(root), dtype=bool)\n",
        "\n",
        "    dom = ET.parse(fn)\n",
        "    root = dom.getroot()\n",
        "    # iterate the sentence\n",
        "    for sx, sent in enumerate(root.iter(\"sentence\")):\n",
        "        # TODO temporary to compare this and transformers\n",
        "        if not [_ for _ in sent.iter(\"aspectTerm\")]:\n",
        "            take[sx] = False\n",
        "            continue\n",
        "        text = sent.find('text').text.lower()\n",
        "        # tokenize the current sentence\n",
        "        token = word_tokenize(text)\n",
        "\n",
        "        # write word index and tag in train_X\n",
        "        try:\n",
        "            for wx, word in enumerate(token):\n",
        "                train_X[sx, wx] = word_idx[word]\n",
        "                mask[sx, wx] = 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for ox, apin in enumerate(sent.iter('aspectTerms')):\n",
        "            for ax, opin in enumerate(apin.iter('aspectTerm')):\n",
        "                target, polarity, start, end = opin.attrib['term'], opin.attrib['polarity'], int(\n",
        "                    opin.attrib['from']), int(opin.attrib['to'])\n",
        "                # find word index (instead of str index) if start,end is not (0,0)\n",
        "                if end != 0:\n",
        "                    if start != 0:\n",
        "                        start = len(word_tokenize(text[:start]))\n",
        "                    end = len(word_tokenize(text[:end])) - 1\n",
        "                    # for training only identify aspect word, but not polarity\n",
        "                    train_y[sx, start] = 1\n",
        "                    train_y_polarity[sx, start] = dict_polarity[polarity]\n",
        "                    if end > start:\n",
        "                        # train_y[sx, start + 1:end] = 2\n",
        "                        train_y[sx, start + 1:end] = 1\n",
        "                        train_y_polarity[sx, start + 1:end] = dict_polarity[polarity]\n",
        "\n",
        "    return (train_X[take], mask[take]), train_y[take], train_y_polarity[take]\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            data[k] = v.to(device)\n",
        "        return data\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def loss_fn(pred, mask, label):\n",
        "    label.masked_fill_(~mask, -100)\n",
        "    pred = pred.view(-1, 5)\n",
        "    label = label.view(-1)\n",
        "    loss = torch.nn.functional.cross_entropy(pred, label, weight = torch.tensor([2, 0.3,0.3,0.3,0.3]))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_acc(pred_tags, mask, true_tags):\n",
        "    if isinstance(pred_tags, list):\n",
        "        pred_tags = torch.cat(pred_tags, 0)\n",
        "        mask = torch.cat(mask, 0)\n",
        "        true_tags = torch.cat(true_tags, 0)\n",
        "    pred_tags = pred_tags[mask]\n",
        "    true_tags = true_tags[mask]\n",
        "    acc = (pred_tags == true_tags).sum() / pred_tags.numel()\n",
        "    f1 = f1_score(true_tags.cpu().numpy(), pred_tags.cpu().numpy(), labels=[0, 1, 2, 3, 4], average='weighted')\n",
        "    cm = confusion_matrix(true_tags.cpu().numpy(), pred_tags.cpu().numpy())\n",
        "\n",
        "    return acc, f1, cm\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, gen_emb, domain_emb, num_classes=5, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
        "        self.gen_embedding.weight = torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
        "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
        "        self.domain_embedding.weight = torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
        "        self.conv1 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 5, padding=2)\n",
        "        self.conv2 = torch.nn.Conv1d(gen_emb.shape[1] + domain_emb.shape[1], 128, 3, padding=1)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv4 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "        self.conv5 = torch.nn.Conv1d(256, 256, 5, padding=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(256, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear_ae = torch.nn.Linear(256, 2)\n",
        "\n",
        "        #aspect sentiment analysis\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.tensor(gen_emb, dtype=torch.float))\n",
        "        self.lstm_l = nn.LSTM(gen_emb.shape[1], hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.lstm_r = nn.LSTM(gen_emb.shape[1],hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.dense = nn.Linear(128*2, 4)\n",
        "\n",
        "\n",
        "    def forward(self, x_train):\n",
        "    \n",
        "        x_emb = torch.cat((self.gen_embedding(x_train), self.domain_embedding(x_train)), dim=2)\n",
        "        x_emb = self.dropout(x_emb).transpose(1, 2)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(torch.cat((self.conv1(x_emb.float()), self.conv2(x_emb.float())), dim=1))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv3(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv4(x_conv))\n",
        "        x_conv = self.dropout(x_conv)\n",
        "\n",
        "        x_conv = torch.nn.functional.relu(self.conv5(x_conv))\n",
        "        x_conv = x_conv.transpose(1, 2)\n",
        "\n",
        "        x_lstm, (hidden, cell) = self.lstm(x_conv)\n",
        "\n",
        "        x_logit = self.linear_ae(x_lstm)\n",
        "\n",
        "        n1 = len(x_train)\n",
        "        n2 = len(x_train[0])\n",
        "  \n",
        "        output = torch.tensor([[[0]*5]*n2]*n1).float()\n",
        "        output.requires_grad = True\n",
        "\n",
        "        #sentiment analysis\n",
        "        pred_aspect = x_logit.max(-1)[1]\n",
        "\n",
        "        for j, pred in enumerate(pred_aspect):\n",
        "            i = 0\n",
        "            aspect = False\n",
        "            start = 0\n",
        "            n = len(pred)\n",
        "            pred = pred.tolist()\n",
        "            count_aspect = 0\n",
        "            index_lst = []\n",
        "            left_right = []\n",
        "            x_train_word = x_train[j]\n",
        "    \n",
        "            while i < n:\n",
        "    \n",
        "                if aspect == True and pred[i] == 0:\n",
        "                  \n",
        "                    count_aspect = count_aspect + 1\n",
        "                    index_lst.append([start, i-1])\n",
        "                    aspect = False\n",
        "                    #print(x_train_word, 'x_train+word')\n",
        "                    #print(x_train_word[start:])\n",
        "                    right_context = x_train_word[start:].flip(dims=(0,))\n",
        "                    #print(right_context)\n",
        "                    #print(right_context[right_context.nonzero().squeeze().detach()])\n",
        "                    left_right.append([x_train_word[:i], right_context[right_context.nonzero().squeeze().detach()]])\n",
        "\n",
        "                if aspect == False and pred[i] == 1:\n",
        "                    start = i\n",
        "                    aspect = True\n",
        "\n",
        "                i = i + 1\n",
        "\n",
        "            for m in range(count_aspect):\n",
        "                inputs = left_right[m]\n",
        "                index = index_lst[m]\n",
        "                x_l, x_r = inputs[0], inputs[1]\n",
        "\n",
        "                if x_l.dim() == 0 or len(x_l) == 0:\n",
        "                    x_l = torch.tensor([0])\n",
        "                elif len(x_l) ==1:\n",
        "                    x_l = torch.tensor([x_l])\n",
        "                \n",
        "                if x_r.dim() == 0 or len(x_r) == 0:\n",
        "                    x_r = torch.tensor([0])\n",
        "                elif len(x_r) ==1:\n",
        "                    x_r = torch.tensor([x_r])\n",
        "                \n",
        "                x_l, x_r = self.embed(x_l), self.embed(x_r)\n",
        "                _, (h_n_l, _) = self.lstm_l(x_l)\n",
        "                _, (h_n_r, _) = self.lstm_r(x_r)\n",
        "                h_n = torch.cat((h_n_l[0], h_n_r[0]), dim=-1)\n",
        "                out = self.dense(h_n)\n",
        "                with torch.no_grad():\n",
        "                    output[j][index[0]:index[1]+1]= torch.cat((torch.tensor([-10]),out),0)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([1,2,0])\n",
        "a[a!= 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "word_indx = build_vocab(DATA_DIR)\n",
        "fn = DATA_DIR + 'restaurant_emb.vec'\n",
        "res_domain_embedding = gen_np_embedding(fn, word_indx, dim=100, emb=True)\n",
        "\n",
        "    \n",
        "fn = DATA_DIR + 'glove.840B.300d.txt'\n",
        "general_embedding = gen_np_embedding(fn, word_indx, dim=300, emb=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "fn = DATA_DIR + 'Restaurants_Train_v2.xml'\n",
        "(X_train_res, mask_res), y_train_res , y_train_pol = create_train_data_restaurant(fn, word_indx, sent_len=100)\n",
        "X, mask, y , y_pol = X_train_res, mask_res, y_train_res, y_train_pol\n",
        "   \n",
        "X_train, X_valid, mask_train, mask_valid, y_train, y_valid , y_pol_train, y_pol_valid= train_test_split(X, mask, y, y_pol, test_size=VALID_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            " \n",
            "[[0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 3 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#print(X_train[:3])\n",
        "#print(mask_train[:3])\n",
        "print(y_train[:5])\n",
        "print(' ')\n",
        "print(y_pol_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples:1616\n",
            "valid samples:405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.63it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "\ttrain_loss:3.406 valid_loss:3.471\n",
            "\ttrain_acc:72.23% valid_acc:71.56%\n",
            "\ttrain_f1:0.752 valid_f1:0.747\n",
            "\ttrain_confusion_matrix:\n",
            "[[18999  1038  1579  1724    24]\n",
            " [ 1465   145   174   182     2]\n",
            " [  462    18    30    44     0]\n",
            " [  499    49    41    42     1]\n",
            " [   50    13     9    15     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4745  291  415  432    5]\n",
            " [ 353   41   45   48    1]\n",
            " [ 120    3    9    9    0]\n",
            " [ 155   10   13   10    0]\n",
            " [   3    1    3    3    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.61it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 1\n",
            "\ttrain_loss:3.446 valid_loss:3.514\n",
            "\ttrain_acc:71.86% valid_acc:71.01%\n",
            "\ttrain_f1:0.750 valid_f1:0.742\n",
            "\ttrain_confusion_matrix:\n",
            "[[18986  1021  1584  1853    23]\n",
            " [ 1465   160   179   175     4]\n",
            " [  457    29    32    28     2]\n",
            " [  524    47    48    39     0]\n",
            " [   55    10    14     8     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4719  297  423  446    3]\n",
            " [ 370   26   49   43    0]\n",
            " [ 118    4    8   11    0]\n",
            " [ 150   11   12   15    0]\n",
            " [   3    1    2    4    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.60it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2\n",
            "\ttrain_loss:3.449 valid_loss:3.573\n",
            "\ttrain_acc:72.03% valid_acc:70.59%\n",
            "\ttrain_f1:0.752 valid_f1:0.742\n",
            "\ttrain_confusion_matrix:\n",
            "[[19064  1008  1680  1793    20]\n",
            " [ 1480   142   168   151     5]\n",
            " [  440    29    40    34     1]\n",
            " [  503    50    46    46     2]\n",
            " [   49     6    12    14     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4682  317  439  443    7]\n",
            " [ 346   34   62   46    0]\n",
            " [ 117    6   11    7    0]\n",
            " [ 147    8   20   13    0]\n",
            " [   5    1    2    2    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3\n",
            "\ttrain_loss:3.414 valid_loss:3.427\n",
            "\ttrain_acc:72.11% valid_acc:72.06%\n",
            "\ttrain_f1:0.751 valid_f1:0.750\n",
            "\ttrain_confusion_matrix:\n",
            "[[19094   988  1636  1769    25]\n",
            " [ 1505   130   173   168     0]\n",
            " [  477    23    35    27     0]\n",
            " [  499    49    49    45     0]\n",
            " [   45    13    12    10     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4772  258  388  462    8]\n",
            " [ 363   35   48   42    0]\n",
            " [ 115    4   14    8    0]\n",
            " [ 145   12   13   18    0]\n",
            " [   7    1    2    0    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 4\n",
            "\ttrain_loss:3.421 valid_loss:3.509\n",
            "\ttrain_acc:71.92% valid_acc:71.17%\n",
            "\ttrain_f1:0.749 valid_f1:0.746\n",
            "\ttrain_confusion_matrix:\n",
            "[[18974   995  1621  1764    32]\n",
            " [ 1501   129   167   168     3]\n",
            " [  478    28    26    36     1]\n",
            " [  503    51    49    35     0]\n",
            " [   54     8    14    10     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4722  254  401  507    4]\n",
            " [ 346   34   56   52    0]\n",
            " [ 121    5    9    6    0]\n",
            " [ 143   14   17   14    0]\n",
            " [   5    0    2    3    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 5\n",
            "\ttrain_loss:3.477 valid_loss:3.551\n",
            "\ttrain_acc:71.44% valid_acc:70.84%\n",
            "\ttrain_f1:0.747 valid_f1:0.743\n",
            "\ttrain_confusion_matrix:\n",
            "[[18784  1048  1637  1832    24]\n",
            " [ 1473   140   196   166     2]\n",
            " [  448    31    28    43     1]\n",
            " [  516    52    39    39     0]\n",
            " [   44    17     9    15     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4696  278  444  464    6]\n",
            " [ 354   37   54   43    0]\n",
            " [ 122    2   11    6    0]\n",
            " [ 151   11   13   13    0]\n",
            " [   5    1    2    2    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 6\n",
            "\ttrain_loss:3.438 valid_loss:3.456\n",
            "\ttrain_acc:71.87% valid_acc:71.50%\n",
            "\ttrain_f1:0.750 valid_f1:0.746\n",
            "\ttrain_confusion_matrix:\n",
            "[[18931  1043  1574  1813    24]\n",
            " [ 1473   136   178   162     3]\n",
            " [  471    26    36    33     2]\n",
            " [  488    68    48    42     0]\n",
            " [   46    16    15     9     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4754  258  426  444    6]\n",
            " [ 364   27   55   42    0]\n",
            " [ 115    9    9    8    0]\n",
            " [ 152   13   12   11    0]\n",
            " [   7    0    2    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 7\n",
            "\ttrain_loss:3.404 valid_loss:3.582\n",
            "\ttrain_acc:72.17% valid_acc:70.54%\n",
            "\ttrain_f1:0.751 valid_f1:0.742\n",
            "\ttrain_confusion_matrix:\n",
            "[[19054   925  1650  1778    24]\n",
            " [ 1497   130   176   161     0]\n",
            " [  462    23    30    30     0]\n",
            " [  512    51    49    36     0]\n",
            " [   54     9    11    12     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4677  278  449  478    6]\n",
            " [ 346   38   62   42    0]\n",
            " [ 112    5    9   14    1]\n",
            " [ 145   16   14   13    0]\n",
            " [   3    0    4    3    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 8\n",
            "\ttrain_loss:3.420 valid_loss:3.535\n",
            "\ttrain_acc:71.96% valid_acc:70.75%\n",
            "\ttrain_f1:0.749 valid_f1:0.741\n",
            "\ttrain_confusion_matrix:\n",
            "[[18891  1001  1559  1803    16]\n",
            " [ 1497   135   193   157     2]\n",
            " [  460    28    37    33     0]\n",
            " [  500    63    45    39     0]\n",
            " [   45    11    15    14     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4706  261  449  466    6]\n",
            " [ 368   25   59   36    0]\n",
            " [ 115    5    9   12    0]\n",
            " [ 145   18   14   11    0]\n",
            " [   7    0    3    0    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 9\n",
            "\ttrain_loss:3.428 valid_loss:3.513\n",
            "\ttrain_acc:72.01% valid_acc:71.01%\n",
            "\ttrain_f1:0.751 valid_f1:0.743\n",
            "\ttrain_confusion_matrix:\n",
            "[[19007  1012  1599  1782    34]\n",
            " [ 1478   126   169   188     2]\n",
            " [  454    24    33    41     0]\n",
            " [  498    51    52    41     2]\n",
            " [   45    10    13    12     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4719  290  446  429    4]\n",
            " [ 354   30   57   47    0]\n",
            " [ 117    3   12    8    1]\n",
            " [ 152   14   15    7    0]\n",
            " [   5    1    3    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 10\n",
            "\ttrain_loss:3.401 valid_loss:3.443\n",
            "\ttrain_acc:72.19% valid_acc:71.84%\n",
            "\ttrain_f1:0.752 valid_f1:0.750\n",
            "\ttrain_confusion_matrix:\n",
            "[[19043   973  1660  1703    29]\n",
            " [ 1479   146   179   180     2]\n",
            " [  461    33    32    33     2]\n",
            " [  487    57    55    38     0]\n",
            " [   49    10    17    10     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4762  222  431  464    9]\n",
            " [ 351   38   44   54    1]\n",
            " [ 115    3    9   14    0]\n",
            " [ 147    8   18   15    0]\n",
            " [   5    0    4    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 11\n",
            "\ttrain_loss:3.395 valid_loss:3.396\n",
            "\ttrain_acc:72.23% valid_acc:71.90%\n",
            "\ttrain_f1:0.752 valid_f1:0.748\n",
            "\ttrain_confusion_matrix:\n",
            "[[19070   949  1656  1732    19]\n",
            " [ 1491   136   193   147     1]\n",
            " [  460    29    29    34     1]\n",
            " [  514    52    45    34     0]\n",
            " [   46    11    16    14     1]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4791  232  437  425    3]\n",
            " [ 353   21   66   48    0]\n",
            " [ 115    6    9   11    0]\n",
            " [ 153   13   15    7    0]\n",
            " [   7    0    2    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 12\n",
            "\ttrain_loss:3.446 valid_loss:3.478\n",
            "\ttrain_acc:71.83% valid_acc:71.45%\n",
            "\ttrain_f1:0.750 valid_f1:0.746\n",
            "\ttrain_confusion_matrix:\n",
            "[[19018  1041  1627  1803    19]\n",
            " [ 1469   139   182   167     1]\n",
            " [  471    27    31    34     1]\n",
            " [  507    57    46    40     0]\n",
            " [   47    12    14    14     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4741  253  438  452    4]\n",
            " [ 359   34   52   43    0]\n",
            " [ 116    3   12   10    0]\n",
            " [ 151    8   18   11    0]\n",
            " [   7    0    2    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 13\n",
            "\ttrain_loss:3.405 valid_loss:3.575\n",
            "\ttrain_acc:72.13% valid_acc:70.51%\n",
            "\ttrain_f1:0.750 valid_f1:0.742\n",
            "\ttrain_confusion_matrix:\n",
            "[[19036   987  1528  1835    29]\n",
            " [ 1496   140   156   186     1]\n",
            " [  481    16    36    28     0]\n",
            " [  504    56    48    40     0]\n",
            " [   51    14    12    10     1]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4681  290  423  488    6]\n",
            " [ 355   34   59   40    0]\n",
            " [ 111    5    7   17    1]\n",
            " [ 139   12   24   13    0]\n",
            " [   5    1    3    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 14\n",
            "\ttrain_loss:3.411 valid_loss:3.574\n",
            "\ttrain_acc:72.02% valid_acc:70.59%\n",
            "\ttrain_f1:0.750 valid_f1:0.743\n",
            "\ttrain_confusion_matrix:\n",
            "[[19033   995  1588  1793    16]\n",
            " [ 1514   124   180   159     1]\n",
            " [  467    34    22    33     0]\n",
            " [  486    56    60    40     0]\n",
            " [   55     6    11    14     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4682  263  459  476    8]\n",
            " [ 345   40   59   44    0]\n",
            " [ 120    5    7    9    0]\n",
            " [ 143   16   18   11    0]\n",
            " [   6    0    4    0    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 15\n",
            "\ttrain_loss:3.414 valid_loss:3.476\n",
            "\ttrain_acc:72.09% valid_acc:71.59%\n",
            "\ttrain_f1:0.751 valid_f1:0.748\n",
            "\ttrain_confusion_matrix:\n",
            "[[19019  1018  1577  1790    15]\n",
            " [ 1468   126   174   174     0]\n",
            " [  473    21    31    34     1]\n",
            " [  489    55    64    39     0]\n",
            " [   53     9     9    15     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4742  279  408  456    3]\n",
            " [ 356   42   43   47    0]\n",
            " [ 109    4   12   16    0]\n",
            " [ 146   15   16   11    0]\n",
            " [   5    0    2    3    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 16\n",
            "\ttrain_loss:3.432 valid_loss:3.433\n",
            "\ttrain_acc:71.93% valid_acc:71.84%\n",
            "\ttrain_f1:0.750 valid_f1:0.749\n",
            "\ttrain_confusion_matrix:\n",
            "[[18963  1010  1618  1781    32]\n",
            " [ 1472   131   183   170     2]\n",
            " [  454    27    37    35     1]\n",
            " [  497    58    52    31     0]\n",
            " [   51    11    11    14     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4768  268  411  440    1]\n",
            " [ 358   36   48   46    0]\n",
            " [ 113    6    9   13    0]\n",
            " [ 147   14   16   11    0]\n",
            " [   7    0    2    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.60it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 17\n",
            "\ttrain_loss:3.439 valid_loss:3.495\n",
            "\ttrain_acc:71.85% valid_acc:71.42%\n",
            "\ttrain_f1:0.749 valid_f1:0.748\n",
            "\ttrain_confusion_matrix:\n",
            "[[18901   951  1665  1815    19]\n",
            " [ 1484   118   176   175     0]\n",
            " [  454    16    42    37     1]\n",
            " [  514    50    41    38     0]\n",
            " [   53    12    11    10     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4730  249  450  458    1]\n",
            " [ 343   36   61   48    0]\n",
            " [ 111    7   13   10    0]\n",
            " [ 144    9   18   17    0]\n",
            " [   4    0    4    2    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.56it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 18\n",
            "\ttrain_loss:3.465 valid_loss:3.513\n",
            "\ttrain_acc:71.69% valid_acc:71.06%\n",
            "\ttrain_f1:0.749 valid_f1:0.743\n",
            "\ttrain_confusion_matrix:\n",
            "[[18884  1040  1635  1822    27]\n",
            " [ 1492   147   183   173     1]\n",
            " [  440    25    28    37     0]\n",
            " [  491    50    38    41     0]\n",
            " [   53    12    11    11     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4719  276  448  441    4]\n",
            " [ 362   33   51   41    1]\n",
            " [ 114    4    9   14    0]\n",
            " [ 158    8   11   11    0]\n",
            " [   5    1    2    2    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 19\n",
            "\ttrain_loss:3.431 valid_loss:3.482\n",
            "\ttrain_acc:71.97% valid_acc:71.32%\n",
            "\ttrain_f1:0.750 valid_f1:0.747\n",
            "\ttrain_confusion_matrix:\n",
            "[[19023   957  1640  1824    26]\n",
            " [ 1489   141   186   149     2]\n",
            " [  448    29    39    53     1]\n",
            " [  512    43    50    41     0]\n",
            " [   53     8    11    15     0]]\n",
            "\tvalid_confusion_matrix:\n",
            "[[4738  262  441  440    7]\n",
            " [ 346   33   66   43    0]\n",
            " [ 118    7    7    9    0]\n",
            " [ 143   16   18   11    0]\n",
            " [   6    0    3    1    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VALID_BATCH_SIZE = 1024\n",
        "\n",
        "NUM_ASPECT_TAGS = 4\n",
        "\n",
        "dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(mask_train), torch.Tensor(y_pol_train))\n",
        "print(f\"train samples:{len(dataset)}\")\n",
        "train_loader = DataLoader(dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "dataset_valid = TensorDataset(torch.Tensor(X_valid), torch.Tensor(mask_valid), torch.Tensor(y_pol_valid))\n",
        "print(f\"valid samples:{len(dataset_valid)}\")\n",
        "test_loader = DataLoader(dataset_valid, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "model = to_device(Model(general_embedding, res_domain_embedding, num_classes=5), device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(parameters, lr=1e-4)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    train_f1 = []\n",
        "    test_f1 = []\n",
        "\n",
        "    model.train()\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    for data in tqdm(train_loader, total=len(train_loader)):\n",
        "        for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "        feature, mask, label = data\n",
        "        feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits = model(feature)\n",
        "\n",
        "        loss = loss_fn(pred_logits.float(), mask, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        pred_tags = pred_logits.max(-1)[1]\n",
        "        preds.append(pred_tags)\n",
        "        masks.append(mask)\n",
        "        labels.append(label)\n",
        "\n",
        "    avg_train_acc, avg_train_f1, train_cm = cal_acc(preds, masks, labels)\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    preds = []\n",
        "    masks = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, total=len(test_loader)):\n",
        "            for i in range(len(data)):\n",
        "                data[i] = data[i].to(device)\n",
        "            feature, mask, label = data\n",
        "            feature, mask, label = feature.long(), mask.bool(), label.long()\n",
        "            pred_logits = model(feature)\n",
        "            loss = loss_fn(pred_logits, mask, label)\n",
        "            \n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            pred_tags = pred_logits.max(-1)[1]\n",
        "\n",
        "            preds.append(pred_tags)\n",
        "            masks.append(mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    avg_test_acc, avg_test_f1, test_cm = cal_acc(preds, masks, labels)\n",
        "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
        "\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"\\ttrain_loss:{:.3f} valid_loss:{:.3f}\".format(avg_train_loss, avg_test_loss))\n",
        "    print(\"\\ttrain_acc:{:.2%} valid_acc:{:.2%}\".format(avg_train_acc, avg_test_acc))\n",
        "    print(\"\\ttrain_f1:{:.3f} valid_f1:{:.3f}\".format(avg_train_f1, avg_test_f1))\n",
        "    print(f\"\\ttrain_confusion_matrix:\\n{train_cm}\")\n",
        "    print(f\"\\tvalid_confusion_matrix:\\n{test_cm}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "task2_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
