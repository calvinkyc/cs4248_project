{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d01f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99bf534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of aspect tags: 2\n",
      "num of polarity tags: 3\n"
     ]
    }
   ],
   "source": [
    "path = 'data/restaurants_laptop_train_with_pos.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# replace all -1 to 2 since pytorch cannot handle negative\n",
    "# so, 2 now means negative polarity\n",
    "df.polarity = df.polarity.replace(-1,2)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df.loc[:, \"aspect_tag\"] = encoder.fit_transform(df[\"aspect_tag\"])\n",
    "\n",
    "sentences = df.groupby(\"num\")[\"text\"].apply(list).values\n",
    "aspect_tags = df.groupby(\"num\")[\"aspect_tag\"].apply(list).values\n",
    "polarity_tags = df.groupby(\"num\")[\"polarity\"].apply(list).values\n",
    "\n",
    "polarity_unique_values = df.polarity.unique()\n",
    "\n",
    "print('num of aspect tags: {}'.format(len(encoder.classes_)))\n",
    "print('num of polarity tags: {}'.format(len(polarity_unique_values)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abf5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drinks', 'got', 'screwed', 'up', ',', 'she', 'acted', 'put', 'upon', '.']\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[4])\n",
    "print(aspect_tags[4])\n",
    "print(polarity_tags[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af348a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    elif isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        return data\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739a3984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 8\n",
    "MODEL_PATH = \"model.bin\"\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "NUM_ASPECT_TAGS = len(encoder.classes_)\n",
    "NUM_POLARITY_TAGS = len(polarity_unique_values)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175d7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4304\n",
      "28996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the vocab in the dataset\n",
    "vocab = set()\n",
    "for s in sentences:\n",
    "#     print(s)\n",
    "    vocab.update(set(s))\n",
    "print(len(vocab))\n",
    "\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Increase the vocab in the tokenizer\n",
    "tokenizer.add_tokens(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b65241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTagDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sentences, aspect_tags, polarity_tags, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.aspect_tags = aspect_tags\n",
    "        self.polarity_tags = polarity_tags\n",
    "        self.max_length = max_length\n",
    "        self.items_to_replace = set([101, 102])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sentence = self.sentences[idx] # Get a sentence\n",
    "        aspect_tags = self.aspect_tags[idx] # Get the corresponding aspect tags\n",
    "        polarity_tags = self.polarity_tags[idx] # Get the corresponding polarity tags\n",
    "        \n",
    "        sentence_encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length = self.max_length,\n",
    "            return_token_type_ids = True,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensor = 'pt',            \n",
    "        )\n",
    "        aspect_tags_encoding = self.tokenizer.encode_plus(\n",
    "            aspect_tags,\n",
    "            max_length = self.max_length,\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids = False,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = False,\n",
    "            return_tensor = 'pt',            \n",
    "        )     \n",
    "        polarity_tags_encoding = self.tokenizer.encode_plus(\n",
    "            polarity_tags,\n",
    "            max_length = self.max_length,\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids = False,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = False,\n",
    "            return_tensor = 'pt',            \n",
    "        )     \n",
    "        \n",
    "        # To debug if there is any [UNK]\n",
    "#         input_ids = sentence_encoding['input_ids']\n",
    "#         attention_mask = np.logical_not(sentence_encoding['attention_mask'])\n",
    "#         input_ids = np.ma.compressed(np.ma.masked_where(attention_mask, input_ids))        \n",
    "#         token_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#         found = -1\n",
    "#         try:\n",
    "#             found = token_list.index(\"[UNK]\")\n",
    "#         except ValueError:\n",
    "#             found = -1\n",
    "#         if found >= 0:\n",
    "#             print(sentence)\n",
    "#             print(token_list)\n",
    "        \n",
    "        \n",
    "        # We are not learning the two BERT special characters, so replace them to '1' \n",
    "        # i.e. Not Aspect Terms\n",
    "        aspect_tags_encoding['input_ids'] = [1 if x in self.items_to_replace \n",
    "                                             else x for x in aspect_tags_encoding['input_ids']]        \n",
    "        \n",
    "        # We are not learning the two BERT special characters, so replace them to '0' \n",
    "        # i.e. Neutral polarity\n",
    "        polarity_tags_encoding['input_ids'] = [0 if x in self.items_to_replace \n",
    "                                               else x for x in polarity_tags_encoding['input_ids']]        \n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(sentence_encoding['input_ids'], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(sentence_encoding['attention_mask'], dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(sentence_encoding['token_type_ids'], dtype=torch.long),\n",
    "            \"aspect_tags\": torch.tensor(aspect_tags_encoding['input_ids'], dtype=torch.long),\n",
    "            \"polarity_tags\": torch.tensor(polarity_tags_encoding['input_ids'], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def test_dataset():\n",
    "\n",
    "\n",
    "    train_dataset = SentenceTagDataset(tokenizer=tokenizer,\n",
    "                                       sentences=train_sentences,\n",
    "                                       aspect_tags=train_aspect_tags,\n",
    "                                       polarity_tags=train_polarity_tags)\n",
    "\n",
    "    train_data_loader = DeviceDataLoader(torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32), device)    \n",
    "\n",
    "    print(train_dataset[0])\n",
    "    \n",
    "\n",
    "\n",
    "    data = train_dataset[0]\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = np.logical_not(data['attention_mask'])\n",
    "    aspect_tags = data['aspect_tags']\n",
    "    polarity_tags = data['polarity_tags']\n",
    "    input_ids = np.ma.compressed(np.ma.masked_where(attention_mask, input_ids))\n",
    "    aspect_tags = np.ma.compressed(np.ma.masked_where(attention_mask, aspect_tags))\n",
    "    polarity_tags = np.ma.compressed(np.ma.masked_where(attention_mask, polarity_tags))\n",
    "\n",
    "    print(len(input_ids))\n",
    "    # print(len(aspect_tags))\n",
    "    # print(input_ids)\n",
    "    # print(aspect_tags)\n",
    "\n",
    "    # items_to_replace = set([101, 102])\n",
    "    # aspect_tags = [1 if x in items_to_replace else x for x in aspect_tags]\n",
    "    # print(aspect_tags)\n",
    "\n",
    "\n",
    "    print(train_dataset.tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    print(encoder.inverse_transform(aspect_tags))  \n",
    "    print(polarity_tags)  \n",
    "    \n",
    "    for batch in train_data_loader:\n",
    "        print(batch)\n",
    "        input_ids_list = batch['input_ids']\n",
    "        print(input_ids_list.shape)\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d30fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad052c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    masking = mask.view(-1) == 1\n",
    "    pred = output.view(-1, num_labels)\n",
    "    true = torch.where(masking, target.view(-1), \n",
    "                       torch.tensor(cel.ignore_index).type_as(target))\n",
    "    loss = cel(pred, true)\n",
    "    return loss\n",
    "\n",
    "class AspectExtractionModel(nn.Module):\n",
    "    def __init__(self, num_aspect_tags, num_polarity_tags, num_vocab):\n",
    "        super(AspectExtractionModel, self).__init__()\n",
    "        self.num_aspect_tags = num_aspect_tags\n",
    "        self.num_polarity_tags = num_polarity_tags\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-cased\")        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(768, self.num_aspect_tags)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(768, self.num_polarity_tags)\n",
    "        # if the number of vocab has been increased, then need to add the new vector \n",
    "        # at the end of the embedding matrix\n",
    "        self.bert_model.resize_token_embeddings(num_vocab)\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, aspect_tags, polarity_tags):\n",
    "        out, _ = self.bert_model(input_ids, attention_mask = attention_mask, \n",
    "                                 token_type_ids = token_type_ids, return_dict=False)\n",
    "        \n",
    "        tag_out = self.dropout1(out)\n",
    "        tag_out = self.fc1(tag_out)\n",
    "        \n",
    "        pol_out = self.dropout2(out)\n",
    "        pol_out = self.fc2(pol_out)\n",
    "        \n",
    "        loss_tag = loss_fn(tag_out, aspect_tags, attention_mask, self.num_aspect_tags)\n",
    "        loss_pol = loss_fn(pol_out, polarity_tags, attention_mask, self.num_polarity_tags)\n",
    "        loss = (loss_tag + loss_pol) / 2\n",
    "        \n",
    "        s = nn.Softmax(dim=2)\n",
    "        \n",
    "        tag_out = s(tag_out)\n",
    "        pol_out = s(pol_out)\n",
    "        \n",
    "        return tag_out, pol_out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e888395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(pred_tags, true_tags, mask):\n",
    "    \n",
    "    batch = pred_tags.shape[0]\n",
    "    acc = 0\n",
    "    for i in range(batch):\n",
    "        pred_array = pred_tags[i].cpu().detach().numpy()\n",
    "        true_array = true_tags[i].cpu().detach().numpy()\n",
    "        mask_array = mask[i].cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "        # when comparing the accuracy, only compare the portion without the padding\n",
    "        # use the mask to remove the padding\n",
    "        #\n",
    "        # in Bert, mask is created with 0 for padding, so need to flip it around, so 1 \n",
    "        # is padding (then we will can use numpy compressed to remove them later)\n",
    "        mask_array = np.logical_not(mask_array)\n",
    "\n",
    "        # Now, only the portion that the true sentence is left for pred and true to\n",
    "        # calculate the accuracy\n",
    "        pred_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, pred_array))\n",
    "        true_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, true_array))\n",
    "        \n",
    "#         print('i: {}'.format(i))\n",
    "#         print('pred_array: {}'.format(pred_array))\n",
    "#         print('true_array: {}'.format(true_array))\n",
    "#         print('mask_array: {}'.format(mask_array))\n",
    "#         print('pred_masked: {}'.format(pred_masked))\n",
    "#         print('true_masked: {}'.format(true_masked))\n",
    "#         print('='*20)\n",
    "\n",
    "        acc += np.sum(pred_unpadded == true_unpadded) / len(pred_unpadded)\n",
    "    return acc / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883d3507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1580,), (396,), (1580,), (396,), (1580,), (396,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_sentences, test_sentences, \n",
    " train_aspect_tags, test_aspect_tags) = model_selection.train_test_split(\n",
    "    sentences, aspect_tags, random_state = 42, test_size = TEST_SIZE)\n",
    "\n",
    "(_, _, \n",
    " train_polarity_tags, test_polarity_tags) = model_selection.train_test_split(\n",
    "    sentences, polarity_tags, random_state = 42, test_size = TEST_SIZE)\n",
    "\n",
    "train_sentences.shape, test_sentences.shape, train_aspect_tags.shape, test_aspect_tags.shape, train_polarity_tags.shape, test_polarity_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52bef269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Of', 'course', 'this', 'atmosphere', 'is', 'lacking', ',', 'but', 'what', 'do', 'you', 'expect', 'from', 'a', '24', 'hour', 'bagel', 'place', 'anyways', '?']\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[78])\n",
    "print(train_aspect_tags[78])\n",
    "print(train_polarity_tags[78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5b8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AspectExtractionModel(\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30530, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SentenceTagDataset(tokenizer=tokenizer, sentences=train_sentences, \n",
    "                                   aspect_tags=train_aspect_tags,\n",
    "                                   polarity_tags=train_polarity_tags)\n",
    "train_data_loader = DeviceDataLoader(torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=TRAIN_BATCH_SIZE), device)    \n",
    "\n",
    "test_dataset = SentenceTagDataset(tokenizer=tokenizer, sentences=test_sentences, \n",
    "                                  aspect_tags=test_aspect_tags,\n",
    "                                  polarity_tags=test_polarity_tags)\n",
    "test_data_loader = DeviceDataLoader(torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=TEST_BATCH_SIZE), device)   \n",
    "\n",
    "model = to_device(AspectExtractionModel(num_aspect_tags = NUM_ASPECT_TAGS, \n",
    "                                        num_polarity_tags = NUM_POLARITY_TAGS,\n",
    "                                        num_vocab = len(tokenizer)), device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa1753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\calvi\\anaconda3\\envs\\CS4248_python\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.92it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 89.71%; Valid acc: 94.25%\n",
      "Train Loss: 0.27639; Valid Loss: 0.14560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.90it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 94.87%; Valid acc: 95.08%\n",
      "Train Loss: 0.13026; Valid Loss: 0.12790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 96.03%; Valid acc: 95.45%\n",
      "Train Loss: 0.10214; Valid Loss: 0.12635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 96.67%; Valid acc: 95.37%\n",
      "Train Loss: 0.08446; Valid Loss: 0.12842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.82it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 97.04%; Valid acc: 95.65%\n",
      "Train Loss: 0.07502; Valid Loss: 0.12465\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_train_steps = int(len(train_sentences) / TRAIN_BATCH_SIZE * NUM_EPOCHS)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=num_train_steps)\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    model.train()\n",
    "    for data in tqdm(train_data_loader, total=len(train_data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        pred_aspect_tags, pred_polarity_tags, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        true_aspect_tags = data['aspect_tags']\n",
    "        true_polarity_tags = data['polarity_tags']\n",
    "        mask = data['attention_mask']\n",
    "        aspect_acc = cal_acc(torch.argmax(pred_aspect_tags, dim=2), true_aspect_tags, mask)\n",
    "        polarity_acc = cal_acc(torch.argmax(pred_polarity_tags, dim=2), true_polarity_tags, mask)\n",
    "        avg_acc = (aspect_acc + polarity_acc) / 2\n",
    "        train_acc.append(avg_acc)\n",
    "        \n",
    "    model.eval()\n",
    "    for data in tqdm(test_data_loader, total=len(test_data_loader)):\n",
    "        pred_aspect_tags, pred_polarity_tags, loss = model(**data)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        \n",
    "        true_aspect_tags = data['aspect_tags']\n",
    "        true_polarity_tags = data['polarity_tags']\n",
    "        mask = data['attention_mask']\n",
    "        aspect_acc = cal_acc(torch.argmax(pred_aspect_tags, dim=2), true_aspect_tags, mask)\n",
    "        polarity_acc = cal_acc(torch.argmax(pred_polarity_tags, dim=2), true_polarity_tags, mask)\n",
    "        avg_acc = (aspect_acc + polarity_acc) / 2\n",
    "        test_acc.append(avg_acc)\n",
    "        \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
    "    avg_test_loss = sum(test_loss) / len(test_loss)\n",
    "    avg_test_acc = sum(test_acc) / len(test_acc)\n",
    "        \n",
    "    print(\"Train acc: {:.2f}%; Valid acc: {:.2f}%\".format(avg_train_acc*100, avg_test_acc*100))\n",
    "    print(\"Train Loss: {:.5f}; Valid Loss: {:.5f}\".format(avg_train_loss, avg_test_loss))\n",
    "    \n",
    "    if avg_test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_loss = avg_test_loss    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a71c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test(test_dataset, test_data_loader, model, num=5, model_path=None):\n",
    "    if model_path is not None: # load the saved model\n",
    "        print('Loading saved model from: {}'.format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model = to_device(model, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num):\n",
    "            \n",
    "            data = next(iter(test_data_loader))\n",
    "            \n",
    "            pred_aspect_tags, pred_polarity_tags, _ = model(**data)\n",
    "            \n",
    "            \n",
    "            input_ids = data['input_ids']\n",
    "            pred_aspect_tags = torch.argmax(pred_aspect_tags, dim=2)\n",
    "            pred_polarity_tags = torch.argmax(pred_polarity_tags, dim=2)\n",
    "            true_aspect_tags = data['aspect_tags']\n",
    "            true_polarity_tags = data['polarity_tags']\n",
    "            mask = data['attention_mask']\n",
    "            \n",
    "            # Randomly pick a test data from this batch\n",
    "            #\n",
    "            idx = np.random.randint(0,pred_aspect_tags.shape[0],size=1)[0]\n",
    "\n",
    "            ids_array = input_ids[idx].cpu().numpy()\n",
    "            pred_aspect_array = pred_aspect_tags[idx].cpu().numpy()\n",
    "            true_aspect_array = true_aspect_tags[idx].cpu().numpy()\n",
    "            pred_polarity_array = pred_polarity_tags[idx].cpu().numpy()\n",
    "            true_polarity_array = true_polarity_tags[idx].cpu().numpy()\n",
    "            mask_array = mask[idx].cpu().numpy()\n",
    "\n",
    "            # Remove the padding as we do not want to print them\n",
    "            #\n",
    "            mask_array = np.logical_not(mask_array)\n",
    "\n",
    "            # Only print the unpadded portion\n",
    "            ids_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, ids_array))\n",
    "            pred_aspect_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, \n",
    "                                                                       pred_aspect_array))\n",
    "            true_aspect_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, \n",
    "                                                                       true_aspect_array))\n",
    "            pred_polarity_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, \n",
    "                                                                         pred_polarity_array))\n",
    "            true_polarity_unpadded = np.ma.compressed(np.ma.masked_where(mask_array, \n",
    "                                                                         true_polarity_array))\n",
    "            \n",
    "            aspect_acc = np.sum(pred_aspect_unpadded == \n",
    "                         true_aspect_unpadded) / len(pred_aspect_unpadded)\n",
    "            polarity_acc = np.sum(pred_polarity_unpadded == \n",
    "                                  true_polarity_unpadded) / len(pred_polarity_unpadded)\n",
    "            \n",
    "            # let's replace 2 back to -1 for presentation\n",
    "            pred_polarity_unpadded = np.where(pred_polarity_unpadded == 2, -1, \n",
    "                                              pred_polarity_unpadded)\n",
    "            true_polarity_unpadded = np.where(true_polarity_unpadded == 2, -1, \n",
    "                                              true_polarity_unpadded)\n",
    "\n",
    "            print(\"Aspect Acc: {:.2f}%\".format(aspect_acc*100))\n",
    "            print(\"Polarity Acc: {:.2f}%\".format(polarity_acc*100))\n",
    "            print(\"Predicted Aspect:\")\n",
    "            print(encoder.inverse_transform(pred_aspect_unpadded))\n",
    "            print(\"True Aspect:\")\n",
    "            print(encoder.inverse_transform(true_aspect_unpadded))\n",
    "            print(\"Predicted Polarity:\")\n",
    "            print(pred_polarity_unpadded)\n",
    "            print(\"True Polarity:\")\n",
    "            print(true_polarity_unpadded)\n",
    "            print(\"Sentence:\")\n",
    "            print(test_dataset.tokenizer.convert_ids_to_tokens(ids_unpadded))            \n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67842c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model from: model.bin\n",
      "Aspect Acc: 93.94%\n",
      "Polarity Acc: 93.94%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True Polarity:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "Sentence:\n",
      "['[CLS]', 'As', 'soon', 'as', 'I', 'wake', 'up', 'on', 'a', 'saturday', 'or', 'sunday', 'it', 'is', 'the', 'first', 'thing', 'on', 'my', 'mind', 'is', 'when', 'and', 'how', 'I', 'will', 'be', 'getting', 'to', 'fried', 'dumpling', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 100.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "True Polarity:\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Sentence:\n",
      "['[CLS]', 'Try', 'the', 'hot', 'dogs', 'too', ',', 'they', \"'re\", 'snappy', 'and', 'delicious', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 95.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      "True Polarity:\n",
      "[ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'Some', 'servers', 'make', 'you', 'feel', 'like', 'they', 'are', 'doing', 'you', 'a', 'favor', 'to', 'bring', 'you', 'the', 'food', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 95.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      "True Polarity:\n",
      "[ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'Some', 'servers', 'make', 'you', 'feel', 'like', 'they', 'are', 'doing', 'you', 'a', 'favor', 'to', 'bring', 'you', 'the', 'food', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 85.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "True Polarity:\n",
      "[ 0  0  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'We', 'did', \"n't\", 'get', 'drink', 'refills', 'and', 'she', 'did', \"n't\", 'even', 'offer', 'us', 'the', 'option', 'of', 'dessert', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 85.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "True Polarity:\n",
      "[ 0  0  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'We', 'did', \"n't\", 'get', 'drink', 'refills', 'and', 'she', 'did', \"n't\", 'even', 'offer', 'us', 'the', 'option', 'of', 'dessert', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 100.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'AT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "True Polarity:\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Sentence:\n",
      "['[CLS]', 'Try', 'the', 'hot', 'dogs', 'too', ',', 'they', \"'re\", 'snappy', 'and', 'delicious', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 93.33%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "True Polarity:\n",
      "[ 0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'It', 'appears', 'to', 'be', 'the', 'owner', \"'s\", 'first', 'venture', 'and', 'it', 'shows', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 93.33%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'AT' 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'AT' 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      "True Polarity:\n",
      "[ 0  0  0  0  0  0  0  1  0  0  0  0 -1  0  0]\n",
      "Sentence:\n",
      "['[CLS]', 'Bottom', 'line', ':', 'B+', 'for', 'the', 'food', ',', 'F', 'for', 'the', 'service', '.', '[SEP]']\n",
      "\n",
      "Aspect Acc: 100.00%\n",
      "Polarity Acc: 100.00%\n",
      "Predicted Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "True Aspect:\n",
      "['NAT' 'NAT' 'AT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT' 'NAT'\n",
      " 'NAT' 'NAT']\n",
      "Predicted Polarity:\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True Polarity:\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Sentence:\n",
      "['[CLS]', 'The', 'staff', 'was', 'the', 'friendliest', 'that', 'have', 'seen', 'in', 'New', 'York', '.', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_test(test_dataset, test_data_loader, model, num=10, model_path=MODEL_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
